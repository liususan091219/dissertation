%!TEX root =../thesis-ex.tex

\chapter{Assisting Security Decision Making with Natural Language Explanations}
\label{ch:security}

% Life's more fun when you live in the moment :) Happy Snapping!
%\vspace{-0.15in}
\section{Runtime Permission Rationale: Introduction}
\label{sec:intro}

%Android permission system controls user's private information (e.g., location) by engaging users to make security decisions. A successful permission system not only needs to sufficiently protect privacy, but should also support effective decision making by sufficiently educating users about why permissions are requested. 

Mobile security and privacy are two challenging tasks~\cite{journals/tocs/EnckGHTCCJMS14,conf/ccs/FeltCHSW11,conf/soups/FeltHEHCW12,conf/chi/AlmuhimediSSAAG15,conf/huc/LinSALHZ12,conf/soups/LinLSH14,yang2015appcontext}.
Recently user privacy issues gather tremendous attention after the Facebook-Cambridge Analytica data scandal~\cite{facebookleak}. 
Android's current solution for protecting the users' private data resources mainly relies on its sandbox mechanism and the permission system. 
Android permissions control the users' private data resources, e.g., locations and contact lists. 
The permission system regulates an Android app to request permissions, and the app users must grant these permissions before the app can get access to the users' sensitive data. 

In earlier versions of Android, permissions are requested at the installation time.
However, studies~\cite{conf/soups/FeltHEHCW12,conf/huc/LinSALHZ12} show that the install-time requests cannot effectively warn the users about potential security risks. 
The users are often not aware of the fact that permissions are requested, and the users also have poor understandings on the meanings and purposes of using the permissions~\cite{conf/soups/FeltHEHCW12,conf/fc/KelleyCCJSW12}. 
It is a critical task to educate the users by explaining permission purposes so that the users can better understand the purposes~\cite{conf/huc/LinSALHZ12,conf/uss/PanditaXYEX13,clap}.

% latex triming figures: \includegraphics[trim={5cm 0 0 0},clip]{example-image-a}
\begin{figure}[t]
	\vspace{-0.1in}
	\centering
	\subfloat[][Default permission-requesting message for the permission group \correcttexttt{STORAGE} in Android.]{\includegraphics[width=.45\linewidth]{figure/chapter2/intro-fig2.png}\label{fig:warning}}\hfill
	\subfloat[][A runtime-permission-group rationale provided by the app for the permission group \correcttexttt{LOCATION}.]{\includegraphics[width=.45\linewidth]{figure/chapter2/intro-fig1.jpg}\label{fig:rationale}}
	\caption{\label{fig:intro}}
	\vspace{-4ex}
\end{figure}

Since Android 6.0 (Marshmallow), the permission system has been replaced by a new system that requests permission groups~\cite{permgroup} at runtime. An example of runtime-permission-group requests is in Figure~\ref{fig:warning}, where Android shows the default permission-requesting message for the permission group \correcttexttt{STORAGE}\footnote{The permission-requesting message is the message displayed in the permission-requesting dialog (Figure~\ref{fig:warning}). For each permission group, this message is fixed across different apps. For example, the permission-requesting message for \correcttexttt{STORAGE} is \emph{Allow \textbf{appname} to access photos, media and files on your device?}}.
The runtime model has three advantages over the old model. 
(1) It gives the users more warnings than the install-time model. 
(2) It allows the users to control an app's privileges at the permission-group level. 
(3) It gives apps the opportunity to embed their permission-group requests in contexts, so that the requests are self-explanatory. 
For example, in Figure~\ref{fig:warning}, a request for accessing the user's gallery is prompted when she is about to send a Tweet. 

With the runtime-permission system, each Android app can leverage a dialog to provide a customized message for explaining its unique purpose of using the permission group.
In Figure~\ref{fig:rationale}, we show an example of such messages from the \emph{Facebook} app for explaining the purpose of requesting the user's location: ``\emph{Facebook uses this to make some features work...}''. 
Such customized messages are called \emph{runtime-permission-group rationales}. 
Runtime-permission-group rationales are often displayed before or after the permission-requesting messages, or upon the starting of the app. For the rest of this paper, for simplicity, whenever the context refers to a runtime-permission-group rationale or a runtime-permission-group request, we use the term \emph{rationale}, \emph{runtime rationale}, and \emph{permission-group rationale} in short for \emph{runtime-permission-group rationale}; we use the term \emph{permission request(-ing message)} in short for \emph{runtime-permission-group request(-ing message)}.

There are three main reasons why runtime rationales are useful in the new permission system. 
(1) \emph{Challenge in Explaining Background Purposes}. 
Although the runtime system allows permission-group requests to be self-explanatory in contexts, there exist cases where the permission groups are used in the background (e.g., read phone number, SMS)~\cite{Micinski2017UserIA}. 
As a result, there does not exist a user-aware context for asking such permission groups.
(2) \emph{Challenge in Explaining non-Straightforward Purposes}. 
When the purpose of requesting a permission group is not straightforward, such as when the permission group is not for achieving a  primary functionality, the context itself may not be clear enough to explain the purpose. 
For example, when the user is about to send a Tweet (Figure~\ref{fig:warning}), she may not notice that the location permission group is requested. 
(3) \emph{Effectiveness of Natural Language Explanations}. 
Prior work~\cite{conf/huc/LinSALHZ12} shows that the users find the usage of a permission better meets their expectation when the purpose of using such permission is explained with a natural language sentence. Furthermore, user studies~\cite{conf/chi/TanNTNTEW14} on Apple's iOS runtime-permission system also demonstrate that displaying runtime rationales can effectively increase the users' approval rates. 

The effectiveness of explaining permission purposes relies on the contents of the explanation sentences~\cite{conf/huc/LinSALHZ12}. 
Because the rationale sentences are created by apps, the quality of such rationales depends on how individual apps (developers) make decisions for providing rationales. 
Three essential decisions are  (1) which permission group(s) the app should explain the purposes for; (2) for each permission group, what words should be used for explaining the permission group's purpose; (3) how specific the explanation should be.

In this paper, we seek to answer the following questions: (1) what are the common decisions made by apps? (2) how are such decisions aligned with the goal of improving the users' understanding of permission-group purposes? 
To understand the general patterns of apps' permission-explaining behaviors, we conduct the first large-scale empirical study on runtime rationales. 
We collect an Android 6.0+ dataset consisting of 83,244 apps. 
From these apps, we obtain 115,558 rationale sentences. 
Our study focuses on the following five research questions.

{\bf RQ1: Overall Explanation Frequency}. We investigate the overall frequency for apps to explain permission-group purposes with rationales. The result can help us understand whether the developers generally acknowledge the usefulness of runtime rationales, and whether the users are generally warned for the usages of different permission groups. 

{\bf RQ2: Explanation Frequency for non-Straightforward vs. Straightforward Purposes}. Prior work~\cite{conf/codaspy/JingAZH14,conf/huc/LinSALHZ12} finds that the users have different expectations for different permission purposes. The Android official documentation~\cite{shouldshow} suggests that apps provide rationales when the permission group's purposes are not straightforward. Therefore, we investigate whether apps more frequently explain non-straightforward purposes than straightforward ones. The result can help us understand the helpfulness of rationales with the users' understandings of permission-group purposes. 

{\bf RQ3: Incorrect Rationales}. We study the population of rationales where the stated purpose is different from the true purpose, i.e., the rationales are incorrect. 
Such study is related to user expectation, because incorrect rationales may confuse the users and mislead them into making wrong security decisions. 

{\bf RQ4: Rationale Specificity}. How exactly do apps explain purposes of requesting permission groups? How much information do rationales carry? 
Do rationales provide more information than the permission-requesting message? 
Do apps provide more specific rationales for non-straightforward purposes than for straightforward purposes? 

{\bf RQ5: Rationales vs. App Descriptions}. Are apps that provide rationales more likely to explain the same permission group's purpose in the app description than apps that do not provide rationales? 
Are the behaviors of explaining a permission group's purposes consistent in the app description and in rationales? Do more apps explain their permission-group purposes in the app description than in rationales?

% In summary, we study the following five research questions:

% \textbf{RQ1}. How often do apps provide permission rationales?

% \textbf{RQ2}. Do apps provide more rationales for non-straightforward purposes or for straightforward ones?

% \textbf{RQ3}. Do there exist a significant number of incorrect rationales?

% \textbf{RQ4}. Do rationales tend to provide more specific information than the system-provided permission-requesting message?

% \textbf{RQ5}. How does explaining permissions in the rationales relate to explaining the same permissions in the app descriptions?


The rest of this paper is organized as follows. Section~\ref{sec:relwork} introduces background and related work, Section~\ref{sec:data} describes the data collection process. Sections~\ref{sec:rq1}-~\ref{sec:rq5} answer RQ1-RQ5. Sections~\ref{sec:threats}-~\ref{sec:conclusion} discuss threats to validity, implications, and conclusion of our study. 

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!
\section{Background and Related Work}
\label{sec:relwork}

\textbf{Android Permissions and the Least-Privilege Principle}. A previous study~\cite{conf/ccs/FeltCHSW11} shows that compared with attack-performing malware, a more prevalent problem in the Android platform is the \emph{over-privilege} issue of Android permissions: apps often request more permissions than necessary. 
Felt \emph{et al.}~\cite{conf/soups/FeltHEHCW12} evaluate 940 apps and find that one-third of them are over-privileged. 
Existing work leverages  static-analysis techniques~\cite{conf/ccs/FeltCHSW11,conf/ccs/AuZHL12} and dynamic-analysis techniques~\cite{journals/tocs/EnckGHTCCJMS14} to build tools for analyzing whether an app follows the \emph{least-privilege principle}. 
The runtime-permission-group rationales we study are for helping the users make decisions on whether a permission-group request is over-privileged. 

\textbf{User Expectation}. Over time, the research literature on Android privacy has focused on studying whether and how an app's permission usage meets the users' expectation~\cite{conf/huc/LinSALHZ12,conf/icse/HuangZTWL14,conf/uss/PanditaXYEX13,conf/icse/GorlaTGZ14,conf/chi/AlmuhimediSSAAG15,Nissenbaum:2004,conf/uss/WijesekeraBHEWB15,conf/sp/RoesnerKMPWC12,conf/chi/KelleyCS13}. 
In particular, Lin \emph{et al.}~\cite{conf/huc/LinSALHZ12} find that the users' security concern for a permission depends on whether they can expect the permission usage. Jing \emph{et al.}~\cite{conf/codaspy/JingAZH14} further find that even in the same app, the users have different expectations for different permissions. For example, in the \emph{Skype} app, the users find the microphone permission more straightforward than the location permission. The Android official documentation~\cite{shouldshow} also points out this difference and suggests that app developers provide more runtime-permission-group rationales for purposes that are not straightforward to expect. 

The research literature on user expectation can be categorized into three lines of work. The first line of work is on detecting contradictions between the code behavior and the user interface~\cite{conf/icse/HuangZTWL14,conf/wisec/AndowALESX17}. The second line of work is on improving existing interfaces to enhance the users' awareness of permission usages~\cite{conf/chi/AlmuhimediSSAAG15,conf/sp/RoesnerKMPWC12,conf/huc/LiGC16,Nissenbaum:2004,Micinski2017UserIA,conf/uss/WijesekeraBHEWB15}. 
This line of work includes privacy nudging~\cite{conf/chi/AlmuhimediSSAAG15}, access control gadget~\cite{conf/sp/RoesnerKMPWC12}, and mapping between permissions and UI components~\cite{conf/huc/LiGC16}. 
In particular, Nissenbaum \emph{et al.}~\cite{Nissenbaum:2004} first propose the concept of privacy as the \emph{contextual integrity}; 
i.e., the users' decision-making process for privacy relies on the contexts~\cite{Micinski2017UserIA,conf/uss/WijesekeraBHEWB15,chen2013contextual,background}. 
The runtime-permission system incorporates the contextual integrity by allowing apps to ask for permission groups within the context. 
The third line of work is on using natural language sentences to represent or enhance the users' expectation regarding the permission usages~\cite{conf/huc/LinSALHZ12,conf/uss/PanditaXYEX13,conf/icse/GorlaTGZ14,conf/ccs/QuRZCZC14}. 
For example, Lin \emph{et al.}~\cite{conf/huc/LinSALHZ12} find that the users of an app are more comfortable with using the app when the app provides clarifications for the permission purposes than they do not provide such clarifications. 
Pandita \emph{et al.}~\cite{conf/uss/PanditaXYEX13} further extract permission explaining sentences from app descriptions. 
Our study results presented in Section~\ref{sec:rq5} show that apps explain purposes of requesting permission groups more frequently in the rationales than in the description. 

\textbf{Runtime Permission Groups and Runtime Rationales}. Since the launch of the runtime-permission system, another line of work~\cite{conf/soups/BonnePBT17,conf/huc/LinSALHZ12,conf/chi/TanNTNTEW14} (including our work) focuses on the runtime-permission system and the users' decisions on such system. 
In particular, Bonne \emph{et al.}~\cite{conf/soups/BonnePBT17} conduct a study similar to the study by Lin \emph{et al.}~\cite{conf/huc/LinSALHZ12} under the runtime-permission system, showing the users' security decisions in the runtime system also rely on their expectations of the permission usages. 
The closest to our work is the study by Tan \emph{et al.}~\cite{conf/chi/TanNTNTEW14} on the effects of runtime rationales in the iOS system. 
Their user-study results show that rationales can improve the users' approval rates for permission requests and increase the comfortableness for the users to use the app. 
Although they have not observed a significant correlation between the rationale contents and the approval rates, such observations may be due to the fact that only one fake app is examined with limited user feedback. 
As a result, such unrelatedness cannot be trivially generalized to our case. Wijesekera et al.~\cite{conf/sp/WijesekeraBTREW17} redesigns the timing of runtime prompts to reduce the \emph{satisficing} and \emph{habituation} issues~\cite{conf/www/AkhaweAVS13,Wogalter2002ResearchbasedGF,harbach2013sorry,conf/soups/SchaubBDC15}. 
Both Wijesekera \emph{et al.}~\cite{conf/sp/WijesekeraBTREW17} and Olejnik \emph{et al.}~\cite{conf/sp/OlejnikDMHKH17} leverage machine learning techniques to reduce user efforts in making decisions for permission requests. 

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!
\vspace{-0.1in}
\section{Data Collection}
\label{sec:data}

\vspace{-0.07in}
\subsection{Crawling Apps}

Since the launch of Android 6.0, many apps have migrated to support the newer versions of Android. 
To obtain as many Android 6.0+ apps as possible, we crawl apps from the following two sources: 
(1) we crawl the top-500 apps in each category from the Google Play store, obtaining 23,779 apps in total; 
(2) we crawl 482,591 apps from APKPure~\cite{apkpure}, which is another app store with copied apps (same ID, same category, same description, etc.) from the Google Play store\footnote{We are not able to collect all these apps from the Google Play store, due to its anti-theft protection that limits the downloading scale.}. 
From the two sources, we collect 494,758 apps. Among these apps, we find 83,244 apps that 
(1) contain version(s) under Android 6.0+; 
(2) request at least 1 out of the 9 dangerous permission groups (Table~\ref{tab:permission}). 
We use these 83,244 apps as the dataset in this paper\footnote{To the best of our knowledge, this dataset is the largest app collection on runtime rationales; 
it is orders of magnitude larger than other runtime-rationale collections in existing work~\cite{Micinski2017UserIA,conf/chi/TanNTNTEW14}.}. 

\subsection{Annotating Permission-group Rationales}

For each app found in the preceding step, we annotate and extract runtime rationales from the app. 
Same as other static user interface texts, runtime rationales are stored in an app's \correcttexttt{./res/values/strings.xml} file. 
Each line of this file contains a rationale's name and the content of the rationale. 

The size of our dataset dictates that it is intractable to manually annotate all the string variables. 
As a result, we leverage two automatic sentence-annotating techniques: 
(1) keyword matching; 
(2) CNN sentence classifier. 
The automatic annotation is a two-step process. 

\textbf{Annotating Rationales for All Permission Groups}. 
For the first step, we design a keyword matching technique to annotate whether a string variable contains mentions of a permission group. 
More specifically, we assign a binary label to each string variable by matching the variable's name or content against 18 keywords referring to permission groups, including ``\emph{permission}'', ``\emph{rationale}'', and ``\emph{toast}''\footnote{The complete list of the 18 keywords can be found on our project website~\cite{runtimeproj}.}. 
To estimate the recall of keyword matching, we randomly sample 10 apps and inspect their string resource files. 
The result of our inspection shows that such keyword matching found all the rationales in the 10 apps. 

\textbf{Annotating Rationales for the 8 Dangerous Permission Groups}\footnote{In this paper, we skip the \correcttexttt{BODY\_SENSORS} permission group because it contains too few rationales.}. 
For the second step, we use the CNN sentence classifier~\cite{cnn,kim2014convolutional} to annotate the outputs from the first step. The annotations indicate whether each rationale describes 1 of the 9 dangerous permission groups~\cite{permgroup}. 
The 9 permission groups contain 26 permissions. These permission groups' protection levels are dangerous and the purposes of requesting these permission groups are relatively straightforward for the users to understand. 
For each permission group, we train a different CNN sentence classifier. 
We manually annotate 200$\sim$700 rationales as the training examples for each classifier. 
After applying CNN, we estimate the classifier's false positive rate (FP) and false negative rate (FN) by inspecting 100 output examples in each permission group. The average FP (FN) over the 8 permission groups is 5.1\% (6.8\%) and the maximum FP (FN) is 13\% (16\%). In total, CNN annotates 115,558 rationales, which can be found on our project's website~\cite{runtimeproj}. 

\textbf{Discussion}. One caveat of our data collection process is that the rationales in string resource files are only \emph{candidates} for runtime prompts. 
That is, they may not be displayed to the users. 
The reason why we do not study only the actually-displayed rationales is that such study relies on dynamic-analysis techniques, which limit the scale of our study subjects.

\section{RQ1: Overall Explanation Frequency}
\label{sec:rq1}

In the first step of our study, we investigate the proportion of apps that provide permission-group rationales to answer RQ1: how often do apps provide permission-group rationales?  
For each of the 9 permission groups, we count how many apps in our dataset request the permission group; 
we denote this value as \textsf{\#used apps}. Among these apps, we further count how many of them explain the requested permission group's purposes with rationales; 
we denote this value as \textsf{\#explained apps}. Given the two values, we measure the \emph{explanation proportion} of a group of apps:

\begin{mydef}[Explanation proportion]
Given a group of apps, its explanation proportion of a permission group is the proportion of apps in that group to explain the purposes of requesting the permission group,  
i.e., \textsf{\#explained apps} / \textsf{\#used apps}. We denote the explanation proportion as \textsf{\%exp}. 
\end{mydef}

\begin{table}[t]
\centering
\caption{The number of the used apps (the \textsf{\#used apps} column), the explained apps (the \textsf{\#explained apps} column), and the proportion of explained app in the used apps (the \textsf{\%exp} column). We sort the permission groups by \textsf{\#used apps}. \label{tab:permission}}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{permgroup}  & \textsf{\#used} &  \textsf{\#explain}  & \multirow{2}{*}{\textsf{\%exp}} & \textsf{\%exp} \\
& \textsf{apps} & \textsf{-ed apps} & & (top) \\
\hline\hline
\correcttexttt{STORAGE} & 73,031 & 14,668 & 20.2\% & \textbf{28.3\%} \\ \hline
\correcttexttt{LOCATION} & 32,648 & 7,088 & \textbf{21.6\%} & \textbf{30.7\%} \\ \hline
\correcttexttt{PHONE} & 31,198& 2,070 &6.7\% & 11.0\% \\ \hline
\correcttexttt{CONTACTS} & 23,492& 2,607 & 11.1\% & 17.7\%\\ \hline
\correcttexttt{CAMERA} & 16,557& 4,235 &\textbf{25.6\%} & \textbf{37.7\%} \\ \hline
\correcttexttt{MICROPHONE} & 9,130& 2,152 & \textbf{23.5\%} & 28.0\%\\ \hline
\correcttexttt{SMS} & 4,589 & 589  & 12.8\% & 16.0\% \\ \hline
\correcttexttt{CALENDAR} & 2,492 & 357 & 14.2\% & 22.6\%\\ \hline
\correcttexttt{BODY\_SENSORS} & 122  & 16  & 13.1\% & 15.4\% \\ \hline
overall & 83,244 & 19,879 & 23.8\% & 33.9\% \\ \hline
\end{tabular}
\vspace{-0.15in}
\end{table}

In Table~\ref{tab:permission}, we show the values of \textsf{\#used apps}, \textsf{\#explained apps}, and \textsf{\%exp} for each permission group. 
In addition, we compute the \textsf{\%exp} value for only the categorical top-500 apps; we denote this value as \textsf{\%exp} (top). 

{\bf Result Analysis}. From Table~\ref{tab:permission} we can observe three findings.
(1) Overall, 23.8\% apps provide runtime rationale. 
(2) The top-500 apps more frequently explain the purposes of using permission groups than the overall apps do.  
(3) The purposes of the four permission groups \correcttexttt{STORAGE}, \correcttexttt{LOCATION}, \correcttexttt{CAMERA}, and \correcttexttt{MICROPHONE} are more frequently explained than the other five permission groups. 

{\bf Finding Summary for RQ1}. 23.8\% apps provide runtime rationales for their permission-group requests. 
Among all the permission groups, four groups' purposes are explained more often than the other permission groups. This result may imply that app developers are less familiar with the purposes of \correcttexttt{PHONE} and \correcttexttt{CONTACTS}. 

\section{RQ2: Explanation Frequency for Non-straightforward vs. Straightforward Purposes}
\label{sec:rq2}

In the second part of our study, we seek to \emph{quantitatively} answer RQ2: 
do apps provide more rationales for non-straightforward permission-group purposes than for straightforward permission-group purposes? 

It is challenging to \emph{precisely} measure the straightforwardness for understanding the purpose of requesting a permission group. 
The reason for such challenge is that such straightforwardness relies on each user's existing knowledge, which varies from user to user. Therefore, we propose to \emph{approximate} the straightforwardness by measuring the \emph{usage proportion} of a permission group in \emph{a set of apps}:

\begin{mydef}[Usage proportion]
Given a set of apps, its usage proportion (denoted as  \textsf{\%use}) of a permission group  is the proportion of the apps (in this set) that request the permission group. 
\label{def:usagefreq}
\end{mydef}

Our approximation is based on the observation that if a permission group is frequently used by a set of apps, the permission-group purpose in that app set is often also straightforward to understand. 
For example, in a camera app, the users are more likely to understand the purpose of the camera permission group than the location permission group~\cite{shouldshow}; 
meanwhile, our statistics show that camera apps also more frequently request the camera permission group (71.4\%) than the location permission group (27.0\%). 

\begin{table}[t]
\centering
\caption{The app sets for measuring the correlation between the usage proportion and the explanation proportion. The apps in each set share the same purpose (the purpose column) to use the primary permission group (the permgroup column) with the usage proportion (the \textsf{\%use} column).\label{tab:appgroup}}
\begin{tabular}{|c|c|c|c|r|}
\hline
% 985*323  &  176*248  &  393*223  &  189*67  &  173*138  &  105*313  &  24*216  &  12*136  &
% 771*252  &  906*318  &  226*250  &  366*94  &  308*124  &  67*199  &  70*117  &  41*144  & 
% 749*309  &  234*380  &  798*276  &  235*57  &  185*95  &  215*331  &  49*0  &  9*91  &  
% 759*180  &  286*244  &  274*149  &  841*238  &  552*198  &  302*184  &  347*77  &  21*74  & 
% 772*258  &  349*205  &  187*233  &  668*185  &  833*260  &  156*194  &  386*84  &  61*286  &  
% 882*318  &  293*291  &  255*249  &  527*186  &  418*186  &  758*179  &  147*147  &  26*167  &
% 706*164  &  326*83  &  205*92  &  628*167  &  625*211  &  105*154  &  604*170  &  27*0  & 
% 783*179  &  340*186  &  143*186  &  277*133  &  353*142  &  37*182  &  43*77  &  360*222  &
appset & permgroup & purpose & \textsf{\%use} & \textsf{\#apps}\\ \hline\hline
 file mgr& \correcttexttt{STORAGE} & file managing & 95.4\%& 499\\\hline
 video players & \correcttexttt{STORAGE}& store video  & 96.6\% & 1,306\\ \hline
 photography & \correcttexttt{STORAGE}& store photos  & 99.7\% & 3,534\\ \hline\hline
%1: 954*349  &  220*255  &  154*299  &  305*145  &  381*289  &  0*0  &  126*286  &  62*226  & 
%2: 962*338  &  211*207  &  240*361  &  268*89  &  241*86  &  218*291  &  31*200  &  11*67  &
%3: 997*314  &  156*268  &  484*193  &  143*28  &  118*108  &  71*327  &  6*45  &  6*50  &
%all: 985*323  &  176*248  &  393*223  &  189*67  &  173*138  &  105*313  &  24*216  &  12*136  &
% 5319
 maps\&navi & \correcttexttt{LOCATION}& GPS navigation  & 92.6\% & 1,541\\ \hline
 weather & \correcttexttt{LOCATION}& local weather  & 95.4\% & 908\\ \hline
travel\&local  & \correcttexttt{LOCATION}& \makecell{local search} & 87.8\% & 2,647\\ \hline\hline
% 1: 816*244  &  878*316  &  299*255  &  400*88  &  376*115  &  90*218  &  71*134  &  63*138  &  
% 2: 742*378  &  954*367  &  52*298  &  313*99  &  134*41  &  12*182  &  8*0  &  10*222  & 
% 3: 816*244  &  878*316  &  299*255  &  400*88  &  376*115  &  90*218  &  71*134  &  63*138  & 
% all: 771*252  &  906*318  &  226*250  &  366*94  &  308*124  &  67*199  &  70*117  &  41*144  &  
% 5096
lockscreen & \correcttexttt{PHONE} & \makecell{answer call wh\\-en screen locked} & 82.6\% & 425\\ \hline
voip call & \correcttexttt{PHONE} & \makecell{make calls} & 84.9\% & 847\\ \hline
caller id & \correcttexttt{PHONE} & \makecell{caller id} & 92.0\% & 175 \\ \hline\hline
% 1: 774*149  &  205*391  &  256*46  &  826*197  &  155*30  &  24*0  &  144*16  &  19*0  &  
% 2: 751*197  &  327*199  &  282*197  &  849*260  &  750*214  &  442*190  &  451*86  &  22*105  &  
% 3: 686*117  &  474*96  &  211*54  &  920*130  &  886*161  &  251*136  &  663*52  &  34*167  &
% all: 759*180  &  286*244  &  274*149  &  841*238  &  552*198  &  302*184  &  347*77  &  21*74  &  
% 1274
caller id & \correcttexttt{CONTACTS} & \makecell{caller id} & 86.7\% & 196 \\ \hline
mail & \correcttexttt{CONTACTS} & auto complete & 77.1\% & 140 \\ \hline
contacts & \correcttexttt{CONTACTS} & contacts backup & 85.8\% & 259 \\ \hline\hline
% 1: 704*123  &  485*137  &  214*71  &  908*129  &  867*165  &  235*130  &  633*56  &  36*143  &  
% 2: 879*431  &  264*324  &  179*520  &  386*296  &  771*361  &  143*350  &  164*87  &  121*353  &
% 3: 722*176  &  340*216  &  220*140  &  757*168  &  857*216  &  131*147  &  440*114  &  35*222  & 
% all: 772*258  &  349*205  &  187*233  &  668*185  &  833*260  &  156*194  &  386*84  &  61*286  & 
% 461
flashlight & \correcttexttt{CAMERA} & flashlight & 96.6\% & 298 \\ \hline
qrscan & \correcttexttt{CAMERA} & qr scanner & 88.4\% & 155 \\ \hline
camera & \correcttexttt{CAMERA} & selfie\&camera & 71.4\% & 749 \\ \hline\hline
% 1: 282*60  &  134*125  &  966*174  &  272*74  &  111*0  &  77*174  &  131*0  &  7*0  & 
% 2: 755*393  &  245*263  &  884*445  &  239*135  &  342*170  &  58*556  &  19*0  &  39*167  &
% 3: 933*325  &  270*450  &  714*286  &  219*30  &  183*88  &  303*339  &  23*0  &  4*0  &  
% all: 749*309  &  234*380  &  798*276  &  235*57  &  185*95  &  215*331  &  49*0  &  9*91  & 
% 1193
recorder & \correcttexttt{MIC} & voice recorder & 75.7\% & 559 \\ \hline
video chat & \correcttexttt{MIC} & video chat & 77.0\% & 139 \\ \hline\hline
% 1: 880*317  &  240*284  &  123*174  &  492*193  &  386*153  &  757*168  &  111*161  &  27*133  &
% 2: 892*331  &  525*315  &  813*310  &  676*191  &  554*286  &  770*243  &  317*136  &  22*333  &  
% all: 882*318  &  293*291  &  255*249  &  527*186  &  418*186  &  758*179  &  147*147  &  26*167  &
% 693 
sms & \correcttexttt{SMS} & sms & 60.4\% & 379\\ \hline
% all: 706*164  &  326*83  &  205*92  &  628*167  &  625*211  &  105*154  &  604*170  &  27*0  & 
calendar & \correcttexttt{CALEND} & calendar & 36.0\% & 300 \\ \hline
% all: 783*179  &  340*186  &  143*186  &  277*133  &  353*142  &  37*182  &  43*77  &  360*222  &
\end{tabular}
\vspace{-0.15in}
\end{table}

\begin{figure} 
\vspace{-0.15in}
    \centering
  \subfloat{%
   \vspace{-0.2in}
       \includegraphics[width=0.85\linewidth]{figure/chapter2/use_mat.pdf}}\\
    \vspace{-0.15in}
     \subfloat{%
       \includegraphics[width=0.85\linewidth]{figure/chapter2/exp_mat.pdf}}
\caption{The usage proportion (top) and the explanation proportion (bottom) of the app sets in Table~\ref{tab:appgroup}. Each element at ($Q$, $P$) shows the proportion of apps in set $Q$ to use/explain the purpose of permission group $P$.\label{fig:mat}}
\vspace{-0.2in}
\end{figure}

To answer RQ2, we first introduce the definitions of the primary permission group. 

\begin{mydef}[Primary Permission Group]
Given a set of apps that share the same primary functionality, if any app relies on (does not rely on) requesting a permission group to achieve that primary functionality, we say that this permission group is a primary (non-primary) permission group to this app set, and this app set is a primary (non-primary) app set to this permission group. An example of such primary (non-primary) pairs is GPS navigation apps and \correcttexttt{LOCATION} (\correcttexttt{CAMERA}) permission group. 
\end{mydef}

To study the relation between the straightforwardness of permission-group purposes and explanation proportions, we leverage the following three-step process. (1) For each permission group $P$, we use keyword matching to identify 1$\sim$3 app sets such that $P$ is a primary permission group to these app sets. (2) For each permission group $Q$, we merge its primary app sets to obtain a larger primary app set for $Q$. (3) For each permission group $P$ and the merged app sets for each permission group $Q$, we compute the proportion for app set $Q$ to use/explain $P$, obtaining two 8 $\times$ 8 matrices. We show all the app sets in Table~\ref{tab:appgroup}, and the two matrices in Figure~\ref{fig:mat}. In each matrix in Figure~\ref{fig:mat}, each row corresponds to a merged app set $Q$ and each column corresponds to a permission group $P$. For each row/column, we also compute the average over its off-diagonal elements and show these values in an additional column/row named \textsf{off-Diag}. That is, elements in \textsf{off-Diag} show the average over non-primary permission groups/app sets.

{\bf Why Using Primary Permission Groups?} By introducing primary permission groups, we are able to identify permission-group purposes that are clearly straightforward (Table~\ref{tab:appgroup}), so that the boundaries between straightforward purposes and non-straightforward purposes are relatively well defined. We can observe such boundaries from the usage proportion matrix (Figure~\ref{fig:mat}, top).  
% storage: 0.4414246482539629, 0.008964284305333605
% camera: -0.48974301374955836, 0.028389953739503194
% location: 0.6145077501985943, 0.0013987271797970303
% phone: 0.5070562990412617, 0.06422788746797548
% contact: 0.7579019068029721, 0.0010614524201611319
% mic: 0.17164627894918055, 0.5407508597319843

\begin{table}[t]
\centering
\vspace{-0.05in}
\caption{The Pearson correlation tests of each permission group, between the usage proportion and the explanation proportion on the 35 Play-store app sets. \label{tab:cate}}
\begin{tabular}{|p{0.2cm}|p{0.5cm}|p{0.13cm}|p{0.5cm}|p{0.13cm}|p{0.5cm}|p{0.13cm}|p{0.5cm}|p{0.33cm}|p{0.5cm}|p{0.13cm}|p{0.13cm}|}
\hline
\multicolumn{2}{|c|}{\correcttexttt{STORAGE}}  & \multicolumn{2}{c|}{\correcttexttt{LOC}}  & \multicolumn{2}{c|}{\correcttexttt{PHONE}} & \multicolumn{2}{c|}{\correcttexttt{CONTACT}} & \multicolumn{2}{c|}{\correcttexttt{CAMERA}} & \multicolumn{2}{c|}{\correcttexttt{MIC}} \\ \hline\hline
r & p & r & p &r & p &r & p &r & p &r & p  \\ \hline
.4 & 8e-3 & .6 & 1e-3  & .5 & 6e-2 & .8 & 1e-3 & {-.5} & 2e-2 & .2 & .5\\ \hline
 \end{tabular}
 \vspace{-0.1in}
 \end{table}

{\bf Result Analysis}. We can observe the following findings from the explanation matrix in Figure~\ref{fig:mat} (bottom). 
(1) By comparing every diagonal element with its two \textsf{off-Diag} counterparts, we can observe that the diagonal elements are usually larger, indicating that straightforward permission-group purposes are explained more frequently than non-straightforward ones. 
On the other hand, there exist a few exceptional cases in \correcttexttt{LOCATION}, \correcttexttt{MICROPHONE}, \correcttexttt{SMS}, and \correcttexttt{CALENDAR} where at least one off-diagonal element is larger than the diagonal element, indicating that non-straightforward permission-group purposes are explained more frequently in these cases. 
(2) By comparing the elements in the \textsf{off-Diag} row, we find that the permission groups for which non-straightforward purposes are most explained are \correcttexttt{STORAGE}, \correcttexttt{LOCATION}, \correcttexttt{CAMERA}, and \correcttexttt{MICROPHONE}. 
Such result is consistent with the overall explanation proportions in Table~\ref{tab:permission}.

% to be added to the extended version:
\textbf{Measuring Correlation Over All Apps}. Because the app sets in Table~\ref{tab:appgroup} cover only a subset of apps, we further design the second measurement study to capture all apps in our dataset. 
The second study includes the following two-step process. 
(1) Based on the app categories in the Google Play store, we partition all apps into 35 sets. After the partition, the two permission groups \correcttexttt{SMS} and \correcttexttt{CALENDAR} contain too few rationales in each app set, and therefore we discard these two permission groups. 
(2) For each permission group, we compute all its usage proportions and explanation proportions in the 35 app sets, and test the Pearson correlation coefficient~\cite{pearson} between the usage proportions and explanation proportions. 
In Table~\ref{tab:cate}, we show the results of the Pearson tests. We can observe that 4 out of the 6 tests show significantly positive correlation, i.e., straightforward purposes are usually more frequently explained. 
Such results are generally consistent with the results in Figure~\ref{fig:mat}. 

\textbf{Finding Summary for RQ2}. Overall, apps \emph{have not} provided more runtime rationales for non-straightforward permission-group purposes than for straightforward ones except for a few cases. This result implies that the majority of apps \emph{have not} followed the suggestion from the Android official documentation~\cite{shouldshow} to provide rationales for non-straightforward permission-group purposes.

\section{RQ3: Incorrect Rationales}
\label{sec:rq3}

In the third part of our study, we investigate the correctness of permission-group rationales. 
We seek to answer RQ3: does there exist a significant proportion of runtime rationales where the stated purposes do not match the true purposes?
%what is the extent of incorrect rationales?
%are there rationales that state one purpose but use the permission group for a different purpose?

It is challenging to derive an app's true purpose for requesting a permission group. However, we can coarsely differentiate between purposes by checking the permissions under a permission group. 
Among the 9 permission groups in Android 6.0 and higher versions, 6 permission groups each contain more than one permission~\cite{permgroup}. For example, the \correcttexttt{PHONE} permission group controls the access to phone-call-related sensitive resources, and this permission group contains 9 phone-call-related permissions: \correcttexttt{CALL\_PHONE}, \correcttexttt{READ\_CALL\_LOG}, \correcttexttt{READ\_PHONE\_STATE}, etc. By examining whether the app requests \correcttexttt{READ\_CALL\_LOG} or \correcttexttt{READ\_PHONE\_STATE}, we can differentiate between the purposes of reading the user's call logs and accessing the user's phone number. 

In order to easily identify the mismatches between the stated purpose and the true purpose, we study 3 permission groups consisting of relatively diverse permissions: \correcttexttt{PHONE}, \correcttexttt{CONTACTS}, and \correcttexttt{LOCATION}. 
In particular, each of the 3 groups contains 1 permission such that 90\% apps requesting the group have requested that permission (whereas other permissions in the same group are requested less frequently);  therefore, we name such permission a \emph{basic permission}. 
The basic permissions of \correcttexttt{PHONE}, \correcttexttt{CONTACTS}, and \correcttexttt{LOCATION} are \correcttexttt{READ\_PHONE\_STATE}, \correcttexttt{GET\_ACCOUNTS}, and \correcttexttt{ACCESS\_COARSE\_LOCATION}, respectively. 

\begin{mydef}[Apps with Incorrect Rationales]
We identify two cases for an app to contain incorrect rationale(s): 
(1) all the rationales state that the app requests only the basic permission, but in fact, the app has requested other permissions (in the same permission group); 
(2) the app requests only the basic permission, but it contains some rationales stating that it has requested other permissions (in the same permission group). 
\end{mydef}

How many apps does each of the two incorrect cases contains? Both cases can mislead the user to make wrong decisions. 
For case (1), the user may grant the permission-group request with the belief that she has granted only the basic permission, but in fact she has granted other permissions. 
For case (2), the user may deny the permission-group request, because the stated purpose of such permission group seems to be unrelated to the app's functionality, 
e.g., when a music player app requests the \correcttexttt{READ\_PHONE\_STATE} permission only to pause the music when receiving phone calls, 
the rationale can raise the user's security concern by stating that the music app needs to make a phone call. 
After the user denies the phone permission group, the app also loses the access to pausing the music. 

\begin{table}[t]
\centering
\caption{The upper table shows the criteria for annotating the basic permission and other permissions in the same permission group. The lower table shows the estimated lower bounds on the numbers of apps containing incorrectly stated rationales. \label{tab:lower}}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} & \multicolumn{2}{c|}{\correcttexttt{CONTACTS}} & \multicolumn{2}{c|}{\correcttexttt{PHONE}} & \multicolumn{2}{c|}{\correcttexttt{LOCATION}} \\ \hline\hline
 \multirow{3}{*}{\makecell{annotate\\criterion}} & \makecell{basic per\\-mission\\class (a)} & \multicolumn{2}{c|}{\makecell{google \\account/\\sign in/\\email add\\ dress}} & \multicolumn{2}{c|}{\makecell{pause inc\\ oming call/\\imei/ ident\\ity/ number/\\cellular}} & \multicolumn{2}{c|}{\makecell{coarse loc\\/area/region\\/approximate\\/beacon\\ /country}}\\ \cline{2-8}
 & \makecell{other per-\\missions\\class (b)} & \multicolumn{2}{c|}{\makecell{contacts/\\friends/\\phonebook}}& \multicolumn{2}{c|}{\makecell{make call/\\call phone/\\call logs}}& \multicolumn{2}{c|}{\makecell{driving/\\fine loc/\\coordinate}}\\ \hline\hline
 \multirow{4}{*}{\makecell{incorrect\\apps}} & \multirow{2}{*}{\makecell{case (1)}} & \textsf{\#err} & \textsf{\%err} & \textsf{\#err}& \textsf{\%err} & \textsf{\#err} & \textsf{\%err} \\ \cline{3-8}
 & & 93& 4.6& 139& 11.3& 9& 0.1\\ \cline{2-8}
 & \multirow{2}{*}{\makecell{case (2)}} & \textsf{\#err} & \textsf{\%err} & \textsf{\#err} & \textsf{\%err} & \textsf{\#err} & \textsf{\%err} \\ \cline{3-8}
 & & 76 & 13.2 & 37& 4.2 & 3& 0.6\\ \hline
 \end{tabular}
 \vspace{-0.15in}
 \end{table}

To study the populations of the two preceding incorrect cases, we again leverage the aforementioned CNN sentence classifier~\cite{cnn}. 
We classify each runtime rationale into one of the following three classes: 
(a) the rationale states the purpose of requesting a basic permission; 
(b) the rationale states the purpose of requesting a permission other than the basic permission; (c) neither (a) nor (b). 
For each of the three permission groups, we manually annotate 600$\sim$900 rationales as the training data. 
After we obtain the predicted labels, we manually judge the resulting rationales that are predicted as (a) or (b) to make sure that there do not exist false positive annotations for incorrect case (1) or (2). 
In Table~\ref{tab:lower}, we show the lower-bound estimations (\textsf{\#err} and \textsf{\%err}) of the two incorrect cases' populations. 
We also show the detailed criteria of our annotations for (a) and (b). 
The list of incorrect rationales and their apps can be found on our project website~\cite{runtimeproj}.

{\bf Result Analysis}. From Table~\ref{tab:lower} we can observe that there exist a significant proportion of incorrectly stated runtime rationales, especially in the incorrect case (1) of the phone permission group and the incorrect case (2) of the contacts permission group. 
In contrast, there exist fewer incorrect cases in the location permission group.
The reason for the location permission group to contain fewer incorrect cases may be that the majority of apps claim only the usage of location, without specifying whether the requested location is fine or coarse. 
The contacts and phone permission groups contain more diverse purposes than the location group does, and our study results show that a significant proportion of apps requesting the two groups state the wrong purposes. 
For example, a significant number of FM radio apps state in the rationales that these apps \emph{only} need to use the phone state to pause the radio when receiving incoming calls;  
however, these apps have also requested the \correcttexttt{CALL\_PHONE} permission, indicating that if the user grants the permission group, these apps also gain the access to \emph{making phone calls} within the app. 

{\bf Finding Summary for RQ3}. There exist a significant proportion of incorrect runtime rationales for the \correcttexttt{CONTACTS} and the \correcttexttt{PHONE} permission groups. This result implies that apps may have confused the users by stating the incorrect permission-group purposes for \correcttexttt{PHONE} and \correcttexttt{CONTACTS}. 

\vspace{-0.05in}
\section{RQ4: Rationale Specificity}
\label{sec:rq4}

In the fourth part of our study, we look into the informativeness of runtime rationales. 
In particular, we seek to answer RQ4: do rationales (e.g., the rationale in Figure~\ref{fig:rationale}) provide more specific information than the system-provided permission-requesting messages (e.g., the message in Figure~\ref{fig:warning})?

% Whenever an app requests 1 of the 9 permission groups for the first time, Android displays a fixed system message for each permission group. 
% For example, the permission-requesting message for the \correcttexttt{STORAGE} permission group is always ``\emph{Allow appname to access photos, media, and files on your device?}''. 
% This message states only the fact that the app is requesting the storage permission group, without providing further information on the specific functionality that causes the app to request the permission group. 

\begin{mydef}[Redundant Rationales]
If a runtime rationale states only the fact that the app is requesting the permission group, i.e., it does not provide more information than the permission-requesting message, we say that the rationale is redundant, and otherwise non-redundant. 
\end{mydef}

Among all the runtime rationales, how many are non-redundant ones? How much do the proportions of non-redundant rationales in each permission group vary across  permission groups?

\begin{figure}[t]
\vspace{-0.13in}
\centering
\begin{tikzpicture} [scale=.9]
\begin{groupplot}[group style={group size= 1 by 2},height=5cm,width=7cm]%[ybar stacked,xtick=\empty,]%ytick=\empty]
\nextgroupplot[ybar,symbolic x coords={3,13, 23, 33, 43, 53}, ybar=.05cm, legend style={at={(1.25, 0.9),font=\small},anchor=north, cells={align=left}}, enlarge x limits=0.1, ymin=0, ymax=1.0,ymajorgrids = true,bar width = 4.5,xtick=data,xticklabels={storage, locate, contact, phone, cam, mic}, xticklabel style={rotate=90}]%ytick=\empty]
\addplot[fill=white,draw=black] 
coordinates {(3, 0.5063)  (13, 0.4626) (23, 0.6301) (33, 0.8426) (43, 0.5456)  (53, 0.2000)};
\addplot[fill=blue,draw=blue] 
coordinates {(3, 0.5776) (13, 0.3666) (23, 0.7210) (33, 0.7242) (43, 0.5441)  (53, 0.2320)};
\addplot[fill=black,draw=black] 
coordinates {(3, 0.5389) (13, 0.4251) (23, 0.7020) (33, 0.7744)  (43, 0.5444)  (53, 0.2270)};
\addlegendentry{primary \\permission\\ group}
\addlegendentry{non-primary\\permission\\ group}
\addlegendentry{overall}
\end{groupplot}
\end{tikzpicture}
\vspace{-0.1in}
\caption{The proportions of non-redundant rationales.\label{fig:nonredunt}}
\vspace{-0.22in}
\end{figure}

To study the population of non-redundant rationales, we leverage the named entity tagging (NER) technique~\cite{conf/acl/FinkelGM05}. 
The reason for us to leverage the NER technique is our observation that non-redundant rationales usually use some words to state the more specific purposes than the fact of using the permission group. 
Moreover, these purpose-stating words usually appear in textual patterns. 
As a result, we can leverage such textual patterns to detect non-redundant rationales. 
For example, in the following rationale, the words tagged with ``\emph{S}'' explain the \emph{specific} purpose of using the permission group \correcttexttt{PHONE}, and the words tagged with \emph{\_O} are other words: ``\emph{this\_O radio\_O application\_O would\_O like\_O to\_O use\_O the\_O phone\_O permission\_O to\_S pause\_S the\_S radio\_S when\_S receiving\_S incoming\_S calls\_S}''. We train a different NER tagger for each of the top-6 permission groups in Table~\ref{tab:permission}\footnote{We skip \correcttexttt{SMS} and \correcttexttt{CALENDAR}, because they both contain too few rationales for estimating the proportions of non-redundant rationales.}. 
For each permission group, we manually annotate 200$\sim$1,000 training examples.
To evaluate the performance of our NER tagger, we randomly sample 100 rationales from NER's output for each permission group, and manually judge these sampled rationales. 
Our judgment results show that NER's prediction accuracy ranges from 85\% to 94\%. The lists of redundant and non-redundant rationales tagged by NER can be found on our project website~\cite{runtimeproj}. 
Next, we obtain the proportions of non-redundant rationales in each permission group. We plot these proportions in Figure~\ref{fig:nonredunt}.

{\bf Result Analysis}. We can observe three findings from Figure~\ref{fig:nonredunt} and additional experiments. 
(1) The proportions of redundant runtime rationales range from 23\% to 77\%. 
(2) While the two permission groups \correcttexttt{PHONE} and \correcttexttt{CONTACTS} have the lowest explanation proportions (Figure~\ref{fig:mat}), they have the highest non-redundant proportions. 
The reason why most phone and contacts rationales are non-redundant is that they usually specify whether the permission group is used for the basic permission or other permissions. 
(3) We also study the proportions of non-redundant rationales in the app sets defined in Table~\ref{tab:appgroup}, 
but we have not observed a significant correlation between the usage proportions and the non-redundant proportions. 

{\bf Finding Summary for RQ4}. A large proportion of the runtime rationales have not provided more specific information than the permission-requesting messages. 
The rationales in \correcttexttt{PHONE} and \correcttexttt{CONTACTS} are most likely to explain more specific purposes than the permission-requesting messages. This result implies that a large proportion of the rationales are either unnecessary or should be more specifically explained. 

% storage: -0.08525327402830728, 0.6316625161571402
% camera: 0.26656440354384053, 0.25593154443406246
% location: 0.45826869561977074, 0.024315491461460417
% phone: 0.0208685377317446, 0.9435491980275414
% microphone: 0.45377481535587233, 0.0893242979883921
% contact: -0.1831326212199634, 0.513563943488966

% storage, 0.5063, 0.5776, 0.5389
% camera, 0.5456, 0.5441, 0.5444
% location, 0.4626, 0.3666, 0.4251
% contact, 0.6301, 0.7210, 0.7020
% phone, 0.8426, 0.7242, 0.7744
% microphone, 0.2000, 0.2320, 0.2270

\section{RQ5: Rationales vs. App Descriptions}
\label{sec:rq5}

In the fifth part of our study, we look into the correlation between the runtime rationales and the app description. 
We seek to answer RQ5: how does explaining a permission group's purposes in the runtime rationales relate to explaining the same permission group's purposes in the app description? 
%does there exist any correlation between the explanation behavior in an app's description and its rationale messages? 
Are apps that provide rationales more likely to explain the same permission group's purposes in the app description than apps that do not provide rationales? 

To identify apps that explain the permission-group purposes in the description, we leverage the WHYPER tool and the keyword matching technique~\cite{conf/uss/PanditaXYEX13}. 
WHYPER is a state-of-the-art tool for identifying permission-explaining sentences. 
We apply WHYPER on the \correcttexttt{CONTACTS} and the \correcttexttt{MICROPHONE} permission groups. 
Because WHYPER~\cite{whypertool} does not provide the entire pipeline solution for other frequent permission groups, we use the keyword matching technique to match sentences for another permission group \correcttexttt{LOCATION}. 
Prior work~\cite{clap} also leverages keyword matching for efficient processing.
We show the results in Table~\ref{tab:whyper}.

\begin{table}[t]
\centering
\caption{The number of apps that explain a permission group's purposes in the app description (the \textsf{\#apps descript} column), in the rationales (the \textsf{\#apps rationales} column), in both (the \textsf{\#apps both} column), and the Pearson correlation coefficients between whether an app explains a permission group's purpose in the description vs. rationales (the \textsf{Pearson} column).\label{tab:whyper}}
\begin{tabular}{|r|r|r|r|r|}
\hline
& \textsf{\#apps}  & \textsf{\#apps} & \textsf{\#apps} & \multirow{2}{*}{\textsf{Pearson}} \\ 
& \textsf{descript} & \textsf{rationales} & \textsf{both} & \\ \hline\hline
\correcttexttt{LOCATION}& 5,747 & 7,088 & 2,028 & (0.15, 1.86e-168)\\ \hline
\correcttexttt{CONTACTS} & 1,542 & 2,607 & 394 & (0.12, 1.5e-78)\\ \hline
\correcttexttt{MICROPH} & 957 & 2,152 & 245 & (0.02, 0.12)\\ \hline 
 \end{tabular}
 \vspace{-0.2in}
 \end{table}

{\bf Result Analysis}. From Table~\ref{tab:whyper}, we can observe two findings. 
(1) In two out of the three cases, 
the correlations are significantly positive. 
Therefore, an app that provides runtime rationales is also more likely to explain the same permission group's purpose in the description. 
(2) There exist more apps using runtime rationales to explain the permission-group purposes than apps that use the descriptions. 

{\bf Finding Summary for RQ5}. 
The explanation behaviors in the description and in the runtime rationales are often positively correlated. 
Moreover, more apps use runtime rationales to explain purposes of requesting permission groups than using the descriptions. This result implies that apps' behaviors of explaining permission-group purposes are generally consistent across the descriptions and the rationales.

\section{Threats to Validity}
\label{sec:threats}

%see Section 7.5 for an example of such section: http://taoxie.cs.illinois.edu/publications/tse-icsm04-spectra.pdf

The threats to external validity primarily include the degree to which the studied Android apps or their runtime rationales are representative of true practice.
We collect the Android apps from two major sources, one of which is the Google Play store, the most popular Android app store. 
Such threats could
be reduced by more studies on more Android app stores in future work. 
%
The threats to internal validity are
instrumentation effects that can bias our results. Faults in
the used third-party tools or libraries  might cause such effects. To reduce these
threats, we manually double check the results on 
dozens of Android apps under analysis. Human errors during the inspection of data annotations might also cause such effects. To reduce these threats, at least two authors of this paper independently conduct the inspection, and then compare the inspection results and discuss to reach a consensus if there is any result discrepancy. 
%One threat to
%construct validity is that our study makes use of the
%data traces collected during executions, hoping that these
%precisely capture the internal program states for each
%execution point.

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!

\section{Implications}
\label{sec:discussion}

In this paper, we attain multiple findings for Android runtime rationales. These findings imply that  developers may be less familiar with the purposes of the \correcttexttt{PHONE} and \correcttexttt{CONTACTS} permission groups and some rationales in these groups may be misleading (RQ1 and RQ3); the majority of apps have not followed the suggestion for explaining non-straightforward purposes~\cite{shouldshow} (RQ2); a large proportion of rationales may either be unnecessary or need further details (RQ4); and apps' explanation behaviors are generally consistent across the descriptions and the rationales (RQ5). Such findings suggest that the rationales in existing apps may not be optimized for the goal of improving the users' understanding of permission-group purposes. Based on these implications, we propose two suggestions on the system design of the Android platform.  

\textbf{Official Guidelines or Recommender Systems}. 
It is desirable to offer an official guideline or a recommender system for suggesting which permission-group purposes to explain~\cite{clap}, e.g., on the official Android documentation or embedded in the IDE. 
For example, such recommender system can provide a list of functionalities, so that the developer can select which functionalities are used by the app. 
Based on the developer's selections, the system scans the permission-group requests by the app, and lets the developer know which permission group(s)'s purposes may look non-straightforward to the users. 
In addition, the system can suggest rationales for the developers to adapt or to adopt~\cite{clap}. 

\textbf{Controls over Permissions for the Users}. When a permission group contains multiple permissions, such design increases the challenges and errors in explaining the purposes of requesting such permission group.  
It is interesting to study whether a user actually knows which permission she has granted, e.g., does a weather app use her precise location or not? 
One potential approach to improve the users' understanding of permission-group purposes is to further scale down the permission-control granularity from the user's end. 
For example, the ``permission setting'' in the Android system can display a list showing whether each of the user's \emph{permissions} (instead of permission groups) has been granted; and doing so also gives the users the right to revoke each permission individually. 

\vspace{-0.07in}
\section{Conclusion}
\label{sec:conclusion}

In this paper, we have conducted the first large-scale empirical study on runtime-permission-group rationales. 
We have leveraged statistical analysis for producing five new findings. 
(1) Less than one-fourth of the apps provide rationales; the purposes of using \correcttexttt{PHONE} and \correcttexttt{CONTACTS} are the least explained. 
(2) In most cases, apps explain straightforward permission-group purposes more than non-straightforward ones. 
(3) Two permission groups \correcttexttt{PHONE} and \correcttexttt{CONTACTS} contain significant proportions of incorrect rationales. 
(4) A large proportion of the rationales do not provide more information than the permission-requesting messages. 
(5) Apps' explanation behaviors in the rationales and in the descriptions are positively correlated. 
Our findings indicate that developers may need further guidance on which permission groups to explain the purposes and how to explain the purposes. 
It may also be helpful to grant the users controls over each permission. 

Our study focuses on analyzing natural language rationales. 
Besides the rationales, other UI components (e.g., layout, images/icons, font size) can also affect the users' decision making. 
In future work, we plan to study the effects of runtime-permission-group requests when considering these factors, and study ways to encourage the developers to provide higher-quality warnings than the current ones. 

\noindent \textbf{Acknowledgment}. We thank the anonymous reviewers and Xiaofeng Wang for their useful suggestions. This work was supported in part by NSF CNS-1513939, CNS-1408944,  CCF-1409423, and CNS-1564274. 

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!
\section{CLAP: Introduction}
\label{sec:intro}

Security and privacy on mobile devices has been a challenging task~\cite{journals/tocs/EnckGHTCCJMS14,conf/ccs/FeltCHSW11,conf/soups/FeltHEHCW12,conf/huc/LinSALHZ12,conf/soups/LinLSH14,yang2015appcontext}. Recently user privacy gathered new attentions following the Facebook-Cambridge Analytica data scandal~\cite{facebookleak}. The current solution for user privacy protection on the Android platform mainly relies on a permission mechanism, i.e., apps have to request permissions before getting access to sensitive resources. 
Unfortunately, previous work~\cite{conf/ccs/FeltCHSW11} finds that apps frequently request more permissions than the apps need. 
To reduce users' concerns toward those \emph{over-privileged apps}~\cite{conf/ccs/FeltCHSW11,journals/tocs/EnckGHTCCJMS14} and improve the users' understanding of permission usages~\cite{conf/soups/ChinFSW12,conf/chi/KelleyCS13}, one effective approach is to give the users warnings by showing natural language explanations~\cite{conf/huc/LinSALHZ12}. 
For instance, WHYPER~\cite{conf/uss/PanditaXYEX13} uses app description sentences to explain permissions; Android and iOS also launched their features of runtime permission explanations in 2015 and 2012, respectively. 

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figure/chapter2/intro-crop.pdf}
\caption{An example showing how CLAP assists developers with permission requirements, with the dashed rectangle showing sentences recommended by CLAP.}
\label{fig:intro}
\vspace{-0.2in}
\end{figure}

% describe three challenges in writing a good explanation sentence
Permission explanations are short sentences that state the purpose of using a permission. 
Permission explanations are written by Android developers~\cite{conf/chi/TanNTNTEW14}; 
within our knowledge, there exists no previous work on studying the steps of multi-stakeholder elicitation~\cite{requirementelicitation} or requirements  specification~\cite{requirementspecification} for writing such sentences. Without these steps, can we rely solely on developers' decisions to explain permissions? 
Although there exist many good examples of app explanations, it is unclear whether explanations provided by developers are interpretable from an average user's perspective. 
In particular, three major challenges can reduce the interpretability of an explanation sentence. 
(1) \emph{Technical Jargons}. Due to the domain knowledge owned by the developers but not the average users, the developers' explanations sometimes contain technical jargons/logics hard for the average users to understand. 
For example, app \emph{GeoTimer Lite} explains the location permission as for ``\emph{geofence}''~\cite{geolite}; however, the average users may not know the meaning of geofence, not to say why geofence requires the location permission~\cite{geofence}. 
(2) \emph{Optimal Length}. If the explanation is too short, it is likely ambiguous (e.g., in Figure~\ref{fig:intro}, it is unclear whether ``\emph{store locator}'' refers to a locator outside or inside the store); 
on the other hand, if the explanation is long and wordy, users may choose to skip it. 
It can be challenging for the developers alone to make the decision on the length/degree of detailedness.
(3) \emph{Rare Permission Usage}. Although it is relatively easy to explain commonly acknowledged permission usages, e.g., the location permission in a GPS app, it becomes much more challenging to \emph{clearly} explain rare permission usages. 

After identifying difficulties in explaining permissions, we propose the first study on the requirements specification/discovery of permission explanations, and we call it the process of \emph{permission requirements discovery}. 
In particular, we build a recommender system, which recommends a list of potential requirements for the permission explanation (i.e., sentences from similar apps' descriptions\footnote{Alternatively, we can also use privacy documents and runtime permission messages. However, both data sources are much more scarce than app descriptions. As a result, we choose to use app descriptions.
However, the two data resources are both applicable to the CLAP framework.}) so that developers could refer to the list for improving the interpretability of their explanations. 
In Figure~\ref{fig:intro}, we illustrate how our system helps the developer of an app discover the requirements. 
First, by observing sentence 2 and sentence 4, the developer finds the current explanation ``\emph{store locator}'' ambiguous, and then explicitly specifies indoor/outdoor; 
second, by observing the keyword ``\emph{map}'' in sentence 3, the developer is reminded of the map feature and adds it to the explanation; 
finally, by observing sentence 4, the developer discovers a new feature, i.e., indoor locator, to be added to the app. 

Because our recommender system leverages similar apps' descriptions, we name it CLAP, which is the abbreviation for $\underline{\textbf{C}}$o$\underline{\textbf{L}}$laborative $\underline{\textbf{A}}$pp $\underline{\textbf{P}}$ermission recommendation. 
CLAP uses the following four-step process to recommend a list of candidate sentences. 
First, based on information from the current app (the current app's title, description, permissions, or category), CLAP leverages a text retrieval technique to rank every app from the dataset (Section~\ref{sec:similar}). 
Second, for every top-ranked app, CLAP goes through every sentence in its description text and assesses whether the sentence explains the target permission (Section~\ref{sec:keywords}). 
CLAP further processes matched sentences so that each sentence contains only one explanation (Section~\ref{sec:candidate}). Third, CLAP aggregates text information of the top-K similar apps, and uses the aggregated word values to re-rank the candidate sentences found in the previous step (Section~\ref{sec:vote}). 
Finally, for top re-ranked sentences, CLAP post-processes the sentences to remove duplications and to improve their interpretability (Section~\ref{sec:postprocess}). 

% evaluation
We evaluate CLAP's performance (Section~\ref{sec:exp}) on a large dataset consisting of 1.4 million Android apps. 
First, we examine the relevance of recommended sentences. \tabularnewline
To evaluate the relevance, we extract the purpose-explaining sentences from 916 apps as the gold standard sentences, and compare CLAP-recommended sentences with the gold-standard sentences. 
The evaluation results show that CLAP has a high relevance score compared with existing state-of-the-art approaches~\cite{conf/uss/PanditaXYEX13}. 
Second, we conduct a qualitative study on specific examples, to observe to what extent the CLAP results can help with the interpretability. 
The study results show that CLAP can effectively recommend candidate sentences that are concise, convey specific purposes, and support a diverse choice of re-phrasing for the same purpose. 
These characteristics show great promise of CLAP in helping developers find more interpretable explanations and bridging the knowledge gap between different stakeholders' viewpoints. 


This paper makes the following three main contributions: 
%{\bf Cheng: we need to make at least one contribution relevant to requirements engineering}
\begin{itemize}
  \item We make the first attempt to study the problem of permission requirements discovery, with a focus on  %{\color{red}{
  explaining an app's permission to users. 
%}}
  %especially when such explanation is not found in the description of the app itself.
  % for app developers to explain an app's permission
  \item We propose a novel CLAP framework for addressing the formulated problem by leveraging similar apps' permission-explaining sentences.
  \item We evaluate CLAP on a large dataset and show that CLAP effectively provides highly relevant explaining sentences, showing great promise of CLAP as an assistant for requirements discovery of app-permission explanations. 
\end{itemize}

% The rest of this paper is organized as follows. Section~\ref{sec:tradeoff} motivates the CLAP framework. Sections~\ref{sec:identify} -~\ref{sec:postprocess} introduce the four-step process of the CLAP framework: identifying explaining sentences  (Section~\ref{sec:identify}),  finding similar apps (Section~\ref{sec:similar}),  voting explaining sentences (Section~\ref{sec:vote}), and post-processing resulting sentences (Section~\ref{sec:postprocess}). Section~\ref{sec:exp} presents the evaluation results on three  security-sensitive permissions. Section~\ref{sec:relwork} discusses related work. Finally, Section~\ref{sec:conclusion} discusses future work, limitations of CLAP, and concludes the paper.

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!

\section{Similar-App Ranker}
\label{sec:similar}

For the first step of the CLAP framework, we design a similar-app ranker to find apps (which also use the target permission) that are the most similar to the current app. 

We define the similarity score between the current app $Q$ and candidate app $D$ on the permission $P$ as the linear interpolation of scores in four components, 
i.e., the pairwise similarities between $Q$ and $D$'s descriptions, titles, permissions, and categories:

\vspace{-0.2in}
\begin{eqnarray}
sim(Q, D, P) &=& (\lambda_1 sim_{desc}(Q, D) \nonumber\\
&& +\lambda_2 sim_{title}(Q, D) + \lambda_3 sim_{perm}(Q, D)\nonumber\\
&& +\lambda_4 sim_{cate}(Q, D))\label{eq:simty}
\end{eqnarray}

where the coefficients $\lambda_i$'s control the importance of each component. 
Next, we describe the definitions of each similarity component. 

\subsection{Description Similarity}

To model the similarity between two descriptions, we use Okapi BM25~\cite{FT006}, In contrast, previous work~\cite{conf/icse/GorlaTGZ14} uses the topic modeling technique to capture the similarity between app descriptions. 
The reason why we choose to use a retrieval model for app descriptions is that app descriptions are usually longer texts (on average an app description contains 135 words). 
For long texts, the topic modeling technique would bring two apps together even if they only remotely belong to the same topic (instead of closely related, e.g., email apps and SMS apps are ``similar'' by the topic modeling technique, although they clearly have different functionalities). 
On the other hand, text retrieval models capture more discriminativeness between the descriptions, so they are more suitable for our problem. 

To model the text similarity using BM25, we further capture both the unigrams and bigrams from the description text. 
We stem the description texts before turning them into unigrams and bigrams. \tabularnewline
In addition to stemming, we also carry out the following pre-processing steps, which are standard pre-processing techniques in text retrieval tasks. 
These standard techniques improve the ranking performance by enhancing the discriminativeness of each app description. 

\textbf{Stop-word Removal}. We remove regular English stop words from Python's nltk stop words list~\cite{nltk}, e.g. ``\emph{the}'' and ``\emph{a}.''
Meanwhile, words such as ``\emph{Android},'' ``\emph{application},'' and ``\emph{version}'' should also be treated as stop words, because they can appear in any app. 
We identify a complete list of 294 words. 
We create the list by empirically scanning through the top frequent words, and then manually annotating whether each word can appear in any app, regardless of the context. 
The list can be found on  our project website~\cite{clapproj}.

\textbf{Background-sentence Removal}. A mobile-app description usually contains some sentences that explain common issues, e.g., ``\emph{fixed bug in version 1.7}.'' 
Same as stop words, such sentences are ``stop sentences'', which do not help explain the unique functionality of the app. 
As a result, we implement a remover of common background sentences for mobile apps using 53 regular expressions. 
Same as the creation of stop words, the creation of regular expressions is based on the empirical judgment on whether a sentence can appear in any app, e.g., \code{.*version$\backslash$s$+\backslash$d.*} detects whether a sentence describes a version number. 
The list of regular expressions can be found on our project website~\cite{clapproj}.

After the preceding pre-processing steps, we obtain the BM25 scores between the current app $Q$ and every candidate app $D$ in the dataset. 
To make the description similarity comparable to other similarity components, we normalize the BM25 scores with the maximum BM25 score over all the candidates before plugging the normalized score into Equation~\ref{eq:simty}. 

\subsection{Title Similarity}
\label{sec:titlesim}

An app's description usually offers the most information to capture its similarities with other apps~\cite{conf/icse/GorlaTGZ14}, but if CLAP uses only the descriptions, sometimes it is difficult to retrieve accurate results, due to the noisy components in descriptions that are not fully cleaned in pre-processing\footnote{For example, many app descriptions contain SEO words, which may not be strictly relevant to app functionality.}. 
To this end, app titles can serve as a complement to descriptions in modeling app similarities. 

One challenge in modeling the title similarity is the vocabulary gap between similar words, e.g., ``\emph{alarm}'' and ``\emph{wake up clock},'' mainly because titles are short texts (on average a title contains 2.8 words). 
As a result, we use a different technique to model the title similarity. 
We leverage word embedding vectors~\cite{conf/nips/MikolovSCCD13} (GoogleNews-neg300~\cite{word2vec}) for bridging the vocabulary gap. 
For each pair of apps $Q$ and $D$, we define their title similarity as the average cosine similarity between each word $w_1\in Q$ and each word $w_2\in D$. 
To avoid over-matching unrelated word pairs, we empirically cut the cosine similarities at 0.4 and set them to 0 if their original scores are less than 0.4. 

\subsection{Permission Similarity}

Because app permissions are categorical data, we model the permission similarity as the Jaccard distance between the two permission lists. 
The reason why we incorporate the permission similarity is based on the observation that an app's permissions can reflect its functionality. 
For example, emergency contact apps usually use \correcttexttt{READ\_CONTACTS} and \correcttexttt{ACCESS\_FINE\_LOCATION} at the same time, and the usage of location permission distinguishes these apps from other contact apps. 

Previous work~\cite{conf/icse/GorlaTGZ14} leverages security-sensitive APIs to model the similarity between apps. Security-sensitive APIs are a finer-grained version of Android permissions. 
Although APIs carry more information than the permissions, it is also more challenging to model the API similarity. 
The challenge comes from the fact that developers often use different APIs to achieve the same functionality (e.g., a Stack Overflow post~\cite{getcurrentlocation} shows several different techniques to obtain user location), and use the same API to achieve different functionalities.
As a result, we model only the permission-level similarity and leave the exploration of API similarity for future work. 

\subsection{Category Similarity}

Finally, we capture the category similarity between the two apps. 
The reason for using the category information is that we observe multiple cases where using only the descriptions is ambiguous.
In some cases, the category information can help clarify the apps' functionalities. 
For example, we find two apps whose descriptions are close to each other, and yet one app is a cooking app for cookie recipe while the other app is a business app for selling cookies. 
We represent each category as a TF-IDF vector, which comes from words that appear in the descriptions of apps in the category. 
The similarity between $Q$ and $D$ is defined as the cosine similarity between the two vectors.

\section{Identifying Permission-Explaining Sentences}
\label{sec:identify}

After retrieving similar apps of the current app $Q$, the next step of CLAP is to identify permission-explaining sentences among those similar apps' descriptions. 

Previous work such as WHYPER~\cite{conf/uss/PanditaXYEX13} addresses this problem (of identifying permission-explaining sentences) by matching sentences from the app description against frequent words in the permission's API documents. 
WHYPER uses only the \emph{entire} description sentences to explain the permission. 
In our problem, however, using the entire sentences can be ineffective. 
The reason for such ineffectiveness is that we are using \emph{other} apps' sentences to explain the current app. 
An entire sentence from another app sometimes contains redundant information: while a part of the sentence matches the current app's purpose, the other part does not match it. 
For example, the sentence ``\emph{save the recording as a ringtone and share it with your friends}'' describes the usages of two permissions: \correcttexttt{RECORD\_AUDIO} and \correcttexttt{READ\_CONTACTS}, whereas the current app uses only the first permission. 
If we use the entire sentence to explain the current app, the second part is irrelevant, whereas if we discard the entire sentence, the relevant part is also discarded. 
In such cases, if we break the original sentence into shorter units, the first part will contain only the relevant information. 
CLAP leverages this methodology to break the original sentence into shorter ones so that some of them are more relevant than the original sentence. 
We describe this process in Section~\ref{sec:candidate}. 

\subsection{Breaking Sentences into Individual Purposes}
\label{sec:candidate}

% recursively traverse the parsing tree and split a phrase if it contains at least one conjunction.

To break a sentence into shorter ones, we leverage the Stanford PCFG parser~\cite{Klein2003} to parse each sentence $s$ into a tree $T$. 
In particular, we extract its sub-sentences based on two main observations. 
First, following the aforementioned example, if the sentence contains conjunction(s), we split it at the conjunction(s), and then extract the sub-sentences. 
Second, as discussed in previous work~\cite{conf/uss/PanditaXYEX13,conf/ccs/QuRZCZC14}, permission usages can usually be captured by short verb phrases, e.g., ``\emph{create QR code from contact},'' ``\emph{assign contact ringtone}.'' 
Therefore, we also extract the verb phrases in the sentence. 

After the split, CLAP adds both the original sentence and the shorter sentences into a candidate sentence set, which is then passed on to the next step for identifying permission-explaining sentences. 
We intend to include as many candidate sentences as possible to boost the quality of the finally chosen ones. 
Therefore, when we traverse the parsing tree $T$, we keep all the verb phrases; e.g., if one verb phrase is embedded in another, we include both of them in the candidate set. 

We summarize our candidate-sentence generator in Algorithm~\ref{algo} for a clearer view, where $s(n)$ denotes the phrase (in sentence $s$) corresponding to node $n$. 

\begin{algorithm}
\caption{Constructing Candidate Set\label{algo}}
\DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Sentence $s$ and its tree structure $T$ obtained from constituent parsing~\cite{Klein2003};}
  \Output{Candidate sentences $S$ from $s$;}
  $S\leftarrow\emptyset;$\;
  $S\leftarrow S \cup \{s\}$;\tcp*{add the original sentence}
  \For{node $n$ in T}{
   \If{$n = VP$}{
   		$S\leftarrow S\cup \{s(n)\}$;\tcp*{add verb phrase}
   }
    \If{$n = CC$}{
  		\For{node $n_0$ in $n.parent.children$ and $n_0 != CC$}{
  			$S\leftarrow S\cup \{s(n_0)\}$;\tcp*{break  conjuncts}
  		}
  		}
  }
\end{algorithm}
\vspace{-0.2in}

\subsection{Matching Permission-Explaining Sentences}
\label{sec:keywords}

\textbf{Using Keyword Matching}. After obtaining the candidate sentence set from the preceding step, we use a pre-defined set of rules to match each candidate sentence, and keep only those sentences that address the target permission. 
More specifically, the pre-defined set of rules include keywords and POS tags~\cite{conf/naacl/ToutanovaKMS03}. 
The reason why we leverage the POS tags is to disambiguate between a word's senses based on its tag. 
For example, when the word ``\emph{contact}'' is used as a noun, it usually refers to phone contacts, so it explains \correcttexttt{READ\_CONTACTS}, whereas if it is used as a verb, e.g., ``\emph{contact us through email},'' it does not explain \correcttexttt{READ\_CONTACTS}. 
The pre-defined keywords and POS tags set can be found on our project website~\cite{clapproj}. 

\textbf{Using WHYPER to Match Sentences}. Alternatively, we can use WHYPER in this step. 
The reason why we use the keyword matching is for a low time cost and for real-time processing. 
WHYPER traverses the entire dependency parsing graph. 
This step makes WHYPER run at least 100 times slower than the keyword matching. 
Meanwhile, the size of our data dictates that we need to process tens of millions of sentences for each permission. 
As a result, we use keyword matching to speed up this step. We plan to support WHYPER in future extensions of CLAP.

After the preceding steps, we discard apps that CLAP has not identified any sentences from. 

%!TEX root =../main.tex
\section{Ranking Candidate Explaining Sentences}
\label{sec:vote}

After the preceding steps, CLAP obtains similar apps and candidate permission-explaining sentences. 
Next, CLAP ranks the candidate sentences and recommends the top sentences to the developer. 

\textbf{Why Ranking Sentences?} After obtaining explaining sentences, a straightforward technique for recommending sentences is the greedy technique, i.e., scanning through the app list top-down and extracting the first 5 sentences. 
However, this simple technique makes mistakes for the following two reasons.  
First, due to the noise in the data, the retrieved similar apps inevitably contain false positive ones\footnote{After exploring three retrieval techniques: BM25~\cite{FT006}, language model~\cite{Zhai07noteson}, and vector space model~\cite{361220}, we find that all the techniques generate false positive results. 
Such results are due to noisy components in the app descriptions, e.g., SEO words that are sometimes irrelevant to the primary app functionality. }. 
As a result, it is very likely for the greedy technique to select sentences from a mismatched app; sentences from mismatched apps usually discuss different purposes. 
Second, even if an app is correctly matched, it may still use the same permission for a different purpose. 
For example, an alarm app may use \correcttexttt{ACCESS\_FINE\_LOCATION} for weather report and advertisement at the same time. 

\textbf{Ranking Candidate Sentences with Majority-Voting. } Because the greedy technique could easily recommend false positive sentences, CLAP adopts an alternative technique: 
it builds a large set of candidate sentences by breaking and matching the sentences in the top-K apps (i.e., the preceding steps in Section~\ref{sec:similar}-Section~\ref{sec:identify}), and it then leverages a ranking function to recommend the top-ranked sentences from the candidates. 
The top-ranked sentences are expected to be more likely the true permission usage. 
But we do not know the true permission usage; so how to design the ranking function? 
To answer this question, we get the inspiration from the \emph{majority-voting} principle~\cite{Dawid:Skene:79}. 
In particular, the more frequent an explanation is seen in the data (i.e., the similar apps' explanations), the more likely this explanation is widely accepted by peer developers; 
as a result, the more likely this sentence is describing the true permission usage. 

To adopt the majority-voting principle, we need to find out how frequent each explanation is, or how many votes each sentence receives. 
The votes should not be based on a sentence's exact-matching frequency in the dataset; 
a sentence may have appeared only once, and yet its purpose is repeated many times in other sentences. 
That is to say, votes should reflect the \emph{semantic frequency} of the stated purpose. 
We can estimate the semantic frequency of a sentence by first estimating the semantic frequencies of its words, and then averaging them to get score of the sentence. 

\textbf{Semantic Frequency of a Word}. We may use a word's term frequency to represent its semantic frequency (in the dataset); 
but if so, the top-ranked words would be non-discriminative, even after removing stop words. 
For example, the top-3 most frequent words for \correcttexttt{READ\_CONTACTS} are ``\emph{contact},'' ``\emph{contacts},'' and ``\emph{read}.''

If these words are used to recommend the sentence, they would likely recommend sentences such as ``\emph{to read contacts},'' which does not address any specific purpose. 
As a result, we build a discriminative word-voting function by leveraging the \emph{inverse document frequency} (IDF \cite{idf}) and text summarization techniques. 

We compute the votes for each word with the following two-step process. 
First, we apply a text summarization algorithm~\cite{Mihalcea04TextRank} to turn each app description into a $\langle$word, weight$\rangle$ vector, and compute the average vector over all the top-K similar apps. 
Second, for each $\langle$word, weight$\rangle$ pair in the average vector, we multiply the word's weight by its IDF value in the dataset. 
The resulting vector represents the votes that each word receives. 
The text summarization algorithm is TextRank~\cite{Mihalcea04TextRank}, which is a graph-based algorithm based-on PageRank~\cite{page1999}. 
TextRank takes a document as input, and outputs a $\langle$word, weight$\rangle$ vector by leveraging the affinity of word pairs.

The weight associated with each word represents how much the word connects with other words, or how important it is to the document. 
After obtaining the TextRank scores, we further normalize the weights so that the weights from different apps are comparable to each other. 
%[Tao to check] the votes of a word is defined -> the votes for a word are defined
In summary, the votes for a word are defined as:

\begin{eqnarray}
votes(w) = IDF(w) \times \frac{1}{K}\sum_{k=1}^K \frac{TextRank(w, D_k)}{\underset{w'\in V}\max TextRank(w', D_k)}\label{eq:vote}
\end{eqnarray}

\noindent where $V$ is the vocabulary set and $D_k$ represents the $k$-th similar app retrieved by our app ranker (Section~\ref{sec:similar}). 
Some examples of the top-ranked words are shown in Table~\ref{tab:casestudy}. 
We can see that the most voted words are often strongly related to the true permission usage. 

\textbf{Semantic Frequency of a Sentence}. 
%[Tao to check]
% The votes for each sentence s is the average votes over its words
% => The votes for each sentence s are the average over the votes for each word
The votes for each sentence $s$ are the average over the votes for each word:

\vspace{-0.3in}
\begin{eqnarray*}
votes(s) = \frac{1}{|s|}\sum_{w\in s} votes(w)
\end{eqnarray*}
\vspace{-0.4in}

\section{Postprocessing Permission-Explaining Sentences}
\label{sec:postprocess}

Finally, CLAP post-processes the most voted sentences from the preceding steps. 
The post-processing includes the following two steps.

\textbf{Removing Duplicated Sentences}. After the sentences are ranked by their votes, some sentences may be duplicated. 
To ensure the diversity of the resulting sentences, we use the greedy technique to select the first 5 unique sentences and recommend them to the developer.

\textbf{Adding Direct Mentions of Permissions}. Note that one sentence can most clearly explain the target permission when the sentence \emph{explicitly} mentions the permission's name. 
On the other hand, some sentences contain only \emph{implicit} mentions of the permission usage. 
For example, the sentence ``\emph{send text messages to your contacts}'' explicitly mentions the target permission \correcttexttt{READ\_CONTACTS} while another sentence ``\emph{send text messages}'' only implicitly mentions the permission. 
To improve the interpretability of the resulting sentences, CLAP uses a list of pre-defined rules to rewrite an implicit permission-mentioning sentence into an explicit permission-mentioning sentence. 
For example, ``\emph{send text messages}'' is rewritten to ``\emph{send text message (from/to contact)}.'' 
Our evaluations do not rely on the post-processing. 
However, the post-processing steps intuitively help with the understanding of the resulting sentences. 
The pre-defined rules used for post-processing can be found on our project website~\cite{clapproj}.
 
% After obtaining the recommended sentences, we find that additional post-processing can further improve the interpretability of the output sentence. This touches the discussion on what is the gold standard for explaining a permission.

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!

\section{Evaluation}
\label{sec:exp}

To assess the effectiveness of CLAP, we design experiments to answer an important research question: to what extent can CLAP help developers with improving the interpretability of explanation sentences?

To answer this research question, we need to first validate the relevance of a recommended sentence to the app's permission purpose.
Notice that for assisting the developer in writing explanations, a recommended sentence must first be \emph{relevant} to the current app's permission purpose, i.e., the sentence discusses the same permission purpose as the current app. Otherwise, the sentence would be invalid for helping the developer, wasting the developer's time to read such sentence. 
To evaluate the relevance of recommended sentences, we conduct quantitative studies using two groups of test collections\footnote{A test collection contains a set of $\langle$app, sentence$\rangle$ pairs where the sentence explains the permission usage of the app. } (Section~\ref{sec:text} and Section~\ref{sec:manual}). The first group contains gold-standard permission purposes explicitly annotated by app developers; the second group contains gold-standard sentences annotated by two authors of this paper. 
After evaluating the relevance, we conduct a qualitative study to inspect the interpretability of example recommended sentences (Section~\ref{sec:quality}).

\subsection{Dataset}

We use the PlayDrone dataset~\cite{conf/sigmetrics/ViennotGN14}, which is a snapshot of the Google Play store in November 2014. Our dataset consists of 1.4 million apps in total. 
In order to fairly compare with the state-of-the-art technique for permission explanation, i.e., WHYPER~\cite{conf/uss/PanditaXYEX13}, we study three permissions~\cite{permgroup}: \correcttexttt{READ\_CONTACTS}, \correcttexttt{RECORD\_AUDIO}, and \correcttexttt{ACCESS\_FINE\_LOCATION}\footnote{The reason for us to choose the three permissions is that the WHYPER tool~\cite{conf/uss/PanditaXYEX13} provides full pipelines for only three permissions. For other permissions, although it is possible to complete the full pipeline with our efforts, the comparison against baselines may not be fair. We plan to include more permissions in future work. }. 
We denote the set of apps containing each of the three permissions in a different font: \textsf{CONTACT}, \textsf{RECORD}, and \textsf{LOCATION}. 
We keep only those apps whose descriptions are in English. 
We show the sizes of the three app-sets in Table~\ref{tab:stat}. 
Because the original \textsf{LOCATION} app-set is too large (more than 360,000 apps), we sample 21\% apps from the original set for efficiency.
Column \#Apps of Table~\ref{tab:stat} shows the sizes of the three app-sets. 

\subsection{Extracting Gold-Standard Sentences}
\label{sec:gold}

When measuring the quality of a recommended sentence, the gold-standard sentence is the ideal explaining sentence to compare with. 
Strictly speaking, it is difficult to obtain a large-scale gold-standard test collection without soliciting annotations from the developers themselves. However, we are able to obtain a significant number of gold-standard sentences through (1) discovering a small set of apps where the developers have annotated the permission usages, and (2) manually annotating a collection of explaining sentences. 
We describe the two techniques as below\footnote{All test collections in this paper can be found on our project website~\cite{clapproj}.}. 

\begin{table}[t]
\caption{Sizes of our three app-sets and five test collections: Q$_{authr}$'s, author-annotated explanations; Q$_{dev}$'s, developer-annotated explanations. \label{tab:stat}}
\centering
\begin{tabular}{p{1.3cm}||c||c||c}
\hline
  & app-set & Q$_{authr}$ & Q$_{dev}$\\ \hline
\textsf{CONTACT} & 62,147 &  48 &  160\\ 
\textsf{RECORD} & 75,034  &  48 &  103 \\ 
\textsf{LOCATION} & 76,528 &  N/A & 564 \\ \hline
\end{tabular}
\end{table} 

\textbf{Developer-Annotated Explanations}. In the PlayDrone dataset, we observe that a small number of apps (2\textperthousand) have included permission explanations in their app descriptions. 
For example, app \emph{AlarmMon}~\cite{alarmmon} appends the following sentences to its main body of description: ``\emph{AlarmMon requests access for reasons below...: ... \correcttexttt{ACCESS\_FINE\_LOCATION}: AlarmMon requests access in order to provide the current weather for your location after alarms...}''
After observing a significant number of gold-standard sentences annotated by developers, we find that these sentences appear in a clear textual pattern: these sentences are usually located at the end of the app descriptions, with a capitalized permission name followed by a permission-explaining sentence. 
As a result, we can use regular expressions to automatically extract such sentences from raw description texts (the regular expressions can be found on our project website~\cite{clapproj}). 
We manually inspect a small sample of extracted sentences to double check whether the regular expressions work as expected, and the results of our manual inspection have an average precision of 97\%. 
We use this technique to obtain three test collections for our three permissions, denoted as as Q$_{dev}$'s. 
We show the number of $\langle$app, gold-standard sentence$\rangle$ pairs in each Q$_{dev}$ in Table~\ref{tab:stat}.  

\textbf{Author-Annotated Explanations}. 
Although Q$_{dev}$'s can reflect permission explanations, there exist length biases in Q$_{dev}$'s. 
The average length of app descriptions from Q$_{dev}$'s (330 words) is 2.4 times that of all app descriptions (135 words). 
The reason for such difference is that apps that carefully address permission explanations tend to carefully address the entire app description as well. 
Because CLAP is built on top of text retrieval models, its performance depends on the length of the current app's description. 
In order to observe CLAP's performance on shorter app descriptions, we follow the evaluation technique from previous work~\cite{conf/uss/PanditaXYEX13} to uniformly sample apps from the entire app-set (for each permission), and then manually annotate the gold-standard sentences.
%\footnote{For each permission, we evenly divide apps into four bins based on the length of descriptions. 
	% We randomly sample 12 apps from each bin and manually annotate the 48 apps. 
	% Because some apps do not contain any permission-explanation sentence, we use the greedy strategy here, i.e., we sample more apps and stop annotation after obtaining 48 apps.}. 
Two authors go through each description sentence, independently annotate the sentences that explain the target permission, and discuss to resolve annotation differences if any. In total, the manual efforts involve annotating $\sim$2,000 sentences for each test collection. We denote the author-annotated collections as Q$_{authr}$'s, and show their sizes in Table~\ref{tab:stat}\footnote{Due to significant manual efforts needed in the annotations, we construct only \textsf{CONTACT}$_{authr}$ and \textsf{RECORD}$_{authr}$ without constructing \textsf{LOCATION}$_{authr}$ for the work in this paper.}.  

\textbf{Discussions on the Sizes of Test Collections}. The sizes of our test collections range from 48 to 564, which is relatively small. However, it is also almost intractable to obtain larger collections. First, manual annotations on permission explanations require a reasonable amount of domain knowledge in mobile apps and technologies. As a result, these efforts cannot be trivially replaced by crowd-workers' annotations. Second, we also cannot rely on existing tools for automatic annotations. We test state-of-the-art sentence annotation tools in previous work~\cite{conf/uss/PanditaXYEX13,conf/ccs/QuRZCZC14}. Unfortunately, these tools have large false positive rates\footnote{We evaluate false positive (FP) rates of WHYPER~\cite{whyper} and AutoCog~\cite{conf/ccs/QuRZCZC14} on the WHYPER benchmark. 
	WHYPER has a 20\% FP rate on the  \correcttexttt{READ\_CONTACTS} app-set and 21\% FP rate on the  \correcttexttt{RECORD\_AUDIO} app-set. 
	AutoCog has a 33\% FP rate on the \correcttexttt{READ\_CONTACTS} app-set.}, and therefore the annotated sentences by these tools are not clean enough to serve as gold-standard sentences. In total, our five test collections consist of 916  $\langle$app, gold-standard sentence$\rangle$ pairs.

\begin{table*}[h]
\vspace{-0.1in}
\caption{The quantitative evaluation results of text-similarity scores: JI (average Jaccard index) and WES (average word-embedding similarity). The highest score among the four approaches is displayed in bold, and the second highest score is displayed with a $\dagger$. We also show the p-values of T-tests between the highest score and second highest score, and the p-value is shown in bold if it is significant (less than 0.05). The parameter settings here are $\lambda_1=\lambda_2=0.4$, $\lambda_3=\lambda_4=0.1$, top-K=500.  \label{tab:result}}
\vspace{-0.1in}
\begin{center}
    \begin{tabular}{|c|c||p{0.55cm}|p{0.55cm}|p{0.55cm}||p{0.55cm}|p{0.55cm}|p{0.55cm}||p{0.55cm}|p{0.55cm}|p{0.55cm}||p{0.55cm}|p{0.55cm}|p{0.55cm}||p{0.55cm}|p{0.55cm}|p{0.55cm}|}
    \hline
 \multicolumn{2}{|l||}{\multirow{2}{*}{}} & \multicolumn{3}{c||}{ \textsf{CONTACT}$_{dev}$} & \multicolumn{3}{c||}{\textsf{RECORD}$_{dev}$} & \multicolumn{3}{c||}{ \textsf{LOCATION}$_{dev}$} &\multicolumn{3}{c||}{ \textsf{CONTACT}$_{authr}$} &\multicolumn{3}{c|}{ \textsf{RECORD}$_{authr}$}\\ \cline{3-17}
 \multicolumn{2}{|l||}{} & top1 & top3 & top5 & top1 & top3 & top5 & top1 & top3 & top5 & top1 & top3 & top5 & top1 & top3 & top5\\ \hline
 \multirow{5}{*}{JI} & T+K  & 0.015& 0.015& 0.014 & 0.054 & 0.052 & 0.054 & 0.019$^{\dagger}$ & 0.019$^{\dagger}$ & 0.019$^{\dagger}$ & 0.065$^{\dagger}$& 0.061$^{\dagger}$ & 0.061$^{\dagger}$ & 0.064 &0.069 & 0.069 \\ \cline{2-17}
 & T+W   & 0.023$^{\dagger}$&0.026$^{\dagger}$ & 0.026$^{\dagger}$ & \textbf{0.092} & 0.087$^{\dagger}$ & 0.086$^{\dagger}$ & $\backslash$ & $\backslash$ & $\backslash$ & 0.058 & 0.059&0.055 & 0.118$^{\dagger}$& 0.107$^{\dagger}$ & 0.108$^{\dagger}$ \\ \cline{2-17}
 & R+K &0.013&0.008& 0.008 & 0.042& 0.044 & 0.043 & 0.014 &0.012 & 0.012 & 0.042 & 0.037& 0.043&  0.090 & 0.082 & 0.084 \\ \cline{2-17}
 & CLAP  & \textbf{0.032}&\textbf{0.036}& \textbf{0.037} & 0.091$^{\dagger}$ & \textbf{0.105} & \textbf{0.103} & \textbf{0.027} & \textbf{0.025} & \textbf{0.023} & \textbf{0.186} & \textbf{0.170}& \textbf{0.152} & \textbf{0.133} & \textbf{0.147} & \textbf{0.129} \\ \cline{2-17}
 & p& 0.18 & 0.07 & \textbf{0.03} & $\backslash$ & 0.16 & 0.15 & \textbf{0.04} & \textbf{0.03} & \textbf{0.03} & \textbf{6e-4} & \textbf{7e-5} & \textbf{1e-4}  & 0.065 & 0.06 & 0.27\\\hline\hline
 \multirow{5}{*}{WES} & T+K& 0.012 & 0.013 & 0.012 & 0.041 & 0.040 & 0.040 & 0.014$^{\dagger}$ & 0.014$^{\dagger}$ & 0.014$^{\dagger}$ & 0.040$^{\dagger}$ & 0.040$^{\dagger}$ & 0.039$^{\dagger}$ & 0.033 & 0.040 & 0.040\\ \cline{2-17}
 & T+W & 0.016$^{\dagger}$ & 0.018$^{\dagger}$ & 0.019$^{\dagger}$ & 0.061$^{\dagger}$ & 0.060$^{\dagger}$ & 0.060$^{\dagger}$ & $\backslash$ & $\backslash$ & $\backslash$ & 0.038 & 0.039 & 0.036& 0.056$^{\dagger}$ & 0.051$^{\dagger}$ & 0.050$^{\dagger}$\\ \cline{2-17}
& R+K & 0.012 & 0.010 & 0.010 & 0.039 & 0.035 & 0.038 & 0.010 & 0.010 & 0.010 & 0.025 & 0.027 & 0.031 & 0.045 & 0.041 & 0.043 \\ \cline{2-17}
& CLAP & \textbf{0.031} & \textbf{0.033} & \textbf{0.033} & \textbf{0.079} & \textbf{0.084} & \textbf{0.081} & \textbf{0.025} & \textbf{0.023} & \textbf{0.021} & \textbf{0.114} & \textbf{0.107} & \textbf{0.097}& \textbf{0.070} & \textbf{0.076} & \textbf{0.068}\\ \cline{2-17}
& p& \textbf{3e-4} & \textbf{2e-4} & \textbf{5e-4} & 0.11 & \textbf{9e-3} & \textbf{9e-3} & \textbf{6e-5} & \textbf{3e-6} & \textbf{5e-7} & \textbf{1e-5}& \textbf{5e-7}& \textbf{2e-6} & 0.28 & \textbf{4e-3} & \textbf{0.02}\\ \hline
\end{tabular}
\end{center}
\vspace{-0.1in}
\end{table*}

\subsection{Evaluation Metrics}
\label{sec:metric}

To evaluate the relevance of CLAP-recommended sentences to the gold-standard sentence, we define the following metrics. 

\textbf{SAC}: Sentence accuracy based on manual judgment. After obtaining sentences recommended by CLAP (and sentences recommended by all baselines), we manually judge the accuracy of the results. For each pair of gold-standard sentence $\times$ CLAP-recommended sentence, two authors independently judge whether the sentences in the pair are semantically identical, and discuss to resolve the judgment differences if any\footnote{For example, if gold-standard sentence $s_1$ = ``\emph{this app uses your contacts permission for contact suggestion},'' recommended sentence $s_2$ = ``\emph{to automatically suggest contact},'' and $s_3$ = ``\emph{to read contacts},'' we judge $s_2$ as relevant and $s_3$ as non-relevant.}. This step gives rise to $2\times 48\times 4\times 5 = 1,920$ sentence-pair labels. 

\textbf{AAC}: App accuracy based on manual judgment. In addition to the sentence accuracy, we also evaluate the accuracy of the app where the recommended sentence comes from. The reason to evaluate the app accuracy is that the developer may want to further make sure that the retrieved apps share the same functionality with the current app. For each pair of $\langle$retrieved app, the current app$\rangle$, two authors independently judge whether the apps in the pair share the same functionality, and discuss to resolve judgment differences if any. This step gives rise to $2\times 48\times 4\times 5 = 1,920$ app-pair labels\footnote{For example, for app $a_1$ = ``\emph{group sms},'' $a_2$ = ``\emph{group message},'' and  $a_3$ = ``\emph{sms template},'' we judge the app $a_2$ as relevant and $a_3$ as non-relevant.}.

\textbf{JI}: Average Jaccard index~\cite{jaccard}. We propose to use an automatic evaluation metric. The average Jaccard index measures the average word-token overlap between a recommended sentence and the gold-standard sentence. We remove stop words in both sentences to reduce the matching of non-informative words. 

\textbf{WES}: Average word-embedding similarity. The average Jaccard index measures only the word-token overlaps. To better capture the semantic similarity, we propose to use another automatic metric, the average cosine distance between word embedding representations of the two sentences~\cite{word2vec}, in short as WES. WES shares the same formulation as the title-similarity function in Section~\ref{sec:titlesim}. More precisely, 

\begin{eqnarray*}
WES(s_r, s_g) = \frac{1}{|s_r|}\frac{1}{|s_g|}\sum_{w_1\in s_r, w_2\in s_g} sparse\_cos(w_1, w_2)
\end{eqnarray*}

\noindent where $s_r$ and $s_g$ are the recommended sentence and the gold-standard sentence, respectively. 
$sparse\_cos$ is set to the word2vec similarity (between $w_1$ and $w_2$) if the word2vec similarity is larger than 0.4; otherwise, $sparse\_cos$ is set to 0. 

For each metric, we report the overall average scores over the top-1, top-3, and top-5 recommended sentences. 

\subsection{Alternative Approaches Under Comparison}
\label{sec:baseline}

Because no previous work has focused on the same setting as our problem, we cannot compare CLAP's performance with an end-to-end approach that entirely comes from any previous work. 
%[Tao to check]
% Cheng: I'm not sure whether "comparative approaches" is a good term to use here; perhaps, call it "comparable baseline approaches'? 
% comparative approaches -> comparable baseline approaches
However, we can build baseline approaches by following intuitive strategies to assemble state-of-the-art approaches as below.

\textbf{Top Similar apps + Permission Keywords (T+K)}. 
For the first baseline approach, we go through the same process for ranking apps (Section~\ref{sec:similar}) and matching permission-explaining sentences (Section~\ref{sec:keywords}). 
However, instead of breaking and ranking sentences, this baseline approach scans through the original description sentences top-down and greedily recommends the first 5 sentences matched by our keyword matcher (Section~\ref{sec:keywords}). 

\textbf{Top Similar apps + WHYPER (T+W)}. 
This alternative approach follows the same pipeline as T + K, except that the sentence matching is through WHYPER~\cite{conf/uss/PanditaXYEX13} instead of our keyword matcher. 

\textbf{Random Similar apps + Keywords (R+K)}. 
This alternative approach follows the same pipeline as T + K, except that the sentence selection is not through the greedy way. 
Instead, the recommended sentences are randomly sampled from all the original sentences matched by our keyword matcher. 

% Due to the difference between CLAP's and the above three comparative approaches, when comparing CLAP with them, we can verify the effectiveness of (1) splitting sentences; (2) voting sentences. 

\subsection{Automatic Quantitative Evaluation: Text-Similarity Scores}
\label{sec:text}

For the first step of the quantitative study, we examine the automatic evaluation metrics JI and WES on the five test collections (including 916 gold-standard sentences). 
In Table~\ref{tab:result}, we report the average JI and WES over the top-1, top-3, and top-5 sentences recommended by CLAP and the three baselines. To configure the 
parameter settings for the study, we empirically set the top-K in the majority voting (Section~\ref{sec:vote}) to 500; we empirically set $\lambda_1=\lambda_2=0.4$ and $\lambda_3=\lambda_4=0.1$ in the similar-app ranker (Equation~\ref{eq:simty}), where the $\lambda_i$'s are shared by all the four approaches. 
The reason for us to set larger weights on the titles and  descriptions than on the permissions and categories is that the titles and  descriptions have more discriminative power than the permissions and categories. 

{\bf Result Analysis}. 
To observe CLAP's performance, for each setting in Table~\ref{tab:result}: $\langle$test collection, top-K, metric$\rangle$, we highlight the approach with the highest score (marked in bold) and second highest score (marked with $\dagger$). 
We conduct statistical significance tests, i.e., T-tests~\cite{ttest}, between the two scores. 
We display the p-values of the T-tests. A p-value is highlighted in bold if it shows statistical significance (i.e., p-value less than 0.05). 
We can observe that CLAP has the highest score over all the settings except for $\langle$\textsf{RECORD}$_{dev}$, JI$\rangle$. 
We can also observe that the majority of T-test results are significant. 
The three least significant settings are JI in \textsf{CONTACTS}$_{dev}$, \textsf{RECORD}$_{authr}$, and \textsf{RECORD}$_{authr}$. 
In general, CLAP performs better in WES than JI. 
Because WES captures external knowledge with word embedding vectors while JI captures only the word-token overlaps, WES models the semantic relevance between the recommended sentences more closely. 

On the other hand, when comparing the scores across different top-K values, we can observe that the p-values of the top-5 scores are slightly more robust than those of the top-1 scores. 
This difference can be explained by the fact that each of the top-5 scores is the average over 5 scores while each of the top-1 scores is an individual score.

Among the three baselines, T + W performs better than T + K,  indicating that WHYPER performs better than our keyword matching technique (Section~\ref{sec:keywords}). T + K performs better than R + K, indicating that sentences from the top similar apps are more relevant than those from random similar apps. 

\textbf{Effects of CLAP's Parameters}. 
To study the effects that CLAP's parameters have on its performance, we conduct two experiments where we vary the parameters ($\lambda_i$ and top-K) and examine how the results change with these parameters. 

{$\lambda_i$s}: $\lambda_i$s determine the importance of each component in the similar-app ranker. We study two variants of $\lambda_i$s (while fixing the top-K): (1) excluding app descriptions; (2) excluding titles. 
In Table~\ref{tab:lambda}, we show CLAP's performance in these two settings.
 We can see that excluding the descriptions always hurts the performance, while excluding the titles can improve the performance. 
 This result indicates that app descriptions are more important than app titles for ranking similar apps.

\begin{table}
\centering
\caption{CLAP's WES results of excluding app descriptions  (denoted by ``-desc''), excluding titles (denoted by ``-title''), and including all four components (denoted by ``all'')\label{tab:lambda}}
\begin{tabular}{p{1.8cm}||c||c||c}
\hline
  & -desc & -title & all\\ \hline
\textsf{CONTACT}$_{dev}$ & 0.026 &  \textbf{0.037} & 0.033 \\ 
\textsf{RECORD}$_{dev}$ & 0.035  & 0.077  &  \textbf{0.081} \\ 
\textsf{LOCATION}$_{dev}$ & 0.015 & \textbf{0.024}  & 0.023 \\ \hline
\end{tabular}
\end{table} 

{Top-K}: the top-K determines how many similar apps to use for the majority voting. 
We study the effects of varying the top-K value while keeping the $\lambda_i$s fixed. 
We plot CLAP's performance in Figure~\ref{fig:topk}. 
We can see that the overall WES scores are relatively stable; for location data, the scores slightly increase as the top-K increases.

\textbf{Summary}. 
The main difference between CLAP and the baseline approaches is that CLAP 
(1) breaks the sentences into shorter ones; 
(2) ranks the sentences through majority voting.
This result indicates that the two heuristic strategies are effective in improving the relevance of the resulting sentences. 

\begin{filecontents}{datax.dat}
100,0.03757,0.07687,0.01890
200,0.03641,0.08019,0.02118
300,0.03728,0.08197,0.02197
400,0.03748,0.07994,0.02186
500,0.03332,0.07761,0.02325
\end{filecontents}

\begin{figure}
\vspace{-0.2in}
\begin{minipage}[t]{0.3\linewidth}
    \hspace*{-0.05\linewidth}
\begin{tikzpicture}[scale=0.8]
\begin{axis}[xlabel={top-K},ylabel={WES},height=4.5cm,width=6.5cm, ymode=log, xtick=data, ylabel style = {yshift=-0.5cm, xshift=0.5cm},legend style={at={(1.5, 0.65), font=\fontsize{6}{6}\selectfont },anchor=north}]
% Graph column 2 versus column 0
\addplot[black,mark=o] table[x index=0,y index=1,col sep=comma] {datax.dat};
\addlegendentry{\normalsize{\textsf{CONTACT}$_{dev}$}}% y index+1 since humans count from 1

% Graph column 1 versus column 0    
\addplot[black,mark=x] table[x index=0,y index=2,col sep=comma] {datax.dat};
\addlegendentry{\normalsize{\textsf{RECORD}$_{dev}$}}

\addplot[black,mark=*] table[x index=0,y index=3,col sep=comma] {datax.dat};
\addlegendentry{\normalsize{\textsf{LOCATION}$_{dev}$}}
\end{axis}
\end{tikzpicture}
\end{minipage}
\caption{CLAP's WES results across different K values\label{fig:topk}}
\vspace{-0.1in}
\end{figure}

\begin{table*}[t]
\vspace{-0.2in}
\caption{Example sentences recommended by CLAP\label{tab:casestudy}}
\centering
\setlength{\tabcolsep}{1.5em}
%{\setlength{\extrarowheight}{2pt}
{\normalsize
\begin{tabular}{p{1.4cm}||p{6cm}||p{6.5cm}|| p{0.7cm}}
\hline
  & current app (Q) & CLAP-recommended sentences& $votes(w)$\\ \hline 
 \begin{minipage}{0.1\textwidth}
  \hspace{-0.2in}\multirow{1}{*}{\textsf{CONTACT}$_{dev}$}
  \end{minipage}
    & \begin{minipage}{0.34\textwidth}
    \vspace{0.1in}
    \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.5em}
    \item \textit{app name}: lazy \textbf{love}
    \item \textit{app description}: lazy \textbf{love} allows you to \textbf{send} \textbf{message}s to your friends and \textbf{love}d ones so you don't forget to \textbf{send} to who matters...
    \item \emph{ground truth}: automatically \textbf{send} SMS to contacts at scheduled time
\end{itemize}
\vspace{0.1in}
\end{minipage}
     &\begin{minipage}{0.36\textwidth}
     \vspace{0.1in}
     \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.1em}
    \item to \textbf{send} a scheduled \textbf{message} ( from/to phone contacts ); 
    \item can set the time to \textbf{send message} ( from/to phone contacts ) or email
    \item typed in or \textbf{select}ed from contacts;
    \item randomly \textbf{select}s a \textbf{message} ( from/to phone contacts ) and person from your list to \textbf{send} a \textbf{message}
    \end{itemize} 
    \vspace{0.1in}
    \end{minipage} & 
\begin{minipage}{0.1\textwidth}
\hspace{-0.3in}\makecell{\textbf{love}\\\textbf{send}\\ \textbf{message}\\feel\\ \textbf{text}\\\textbf{select}\\ set}
\end{minipage}
     \\ \hline
  \begin{minipage}{0.1\textwidth}
  \hspace{-0.2in}\multirow{1}{*}{\textsf{RECORD}$_{dev}$}
  \end{minipage} & \begin{minipage}{0.34\textwidth}
    \vspace{0.1in}
    \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.5em}
    \item \textit{app name}: build doc
    \item \textit{app description}: builddoc is an easytouse project based photo documentation application that allows you to capture field issues and \textbf{assign} and mange team member's \textbf{task}se ... 
    \item \emph{ground truth}: to record voice and audio \textbf{notes}
\end{itemize}
\vspace{0.1in}
\end{minipage}
   & \begin{minipage}{0.36\textwidth}
     \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.1em}
    \item creating audio \textbf{notes} using the device microphone ( to  record voice );
    \item use your own ( recorded ) voice to create audio \textbf{note};
    \item record voice \textbf{note}s to explain expenses; 
    \item compose text \textbf{note}s using ( recorded ) speech to text and voice commands;
    \item capture photo of a book and record yourself reading it to your child;
    \end{itemize} 
    \end{minipage}
       & \begin{minipage}{0.1\textwidth}
  \hspace{-0.3in}\makecell{project\\ \textbf{task}\\ upload\\ manage\\ \textbf{assign} \\ \textbf{note}\\ edit} 
  \end{minipage} \\ \hline
 \begin{minipage}{0.1\textwidth}
  \hspace{-0.2in}\multirow{1}{*}{\textsf{LOCATION}$_{dev}$}
  \end{minipage} & \begin{minipage}{0.34\textwidth}
    \vspace{0.05in}
    \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.5em}
    \item \textit{app name}: menards
    \item \textit{app description}: home improvement made easy, \textbf{shop} departments, and more. buy in app or find products at your closest \textbf{store}... 
    \item \emph{ground truth}: to provide local \textbf{store} information and directions from your location
\end{itemize}
\vspace{0.05in}
\end{minipage} & \begin{minipage}{0.36\textwidth}
     \begin{itemize}[leftmargin=*]
    \setlength\itemsep{.1em}
    \item plus find a \textbf{store} near you; 
    \item use the map view to locate \textbf{store}s near you;
    \item to find a location near you;
    \item search and discover different products from \textbf{store}s near you;
    \item map the aisle location of any instock \textbf{item} with the product locator; 
    \end{itemize} 
    \end{minipage} & \begin{minipage}{0.1\textwidth}
  \hspace{-0.3in}\makecell{order\\ reorder\\ \textbf{store}\\ \textbf{shop}\\ \textbf{item} \\ special\\ pickup} 
  \end{minipage} \\ \hline
\end{tabular}}
\vspace{-0.05in}
\end{table*}

\subsection{Quantitative Evaluation: Manually-Judged Accuracy}
\label{sec:manual}


For the second step of the quantitative study, we conduct a manual evaluation on the sentence accuracy (SAC) and app accuracy (AAC). 
This step is for obtaining more interpretable metrics (accuracy) than JI and WES. 
The SAC/AAC scores reflect how high percent of the top resulting sentences/apps are relevant. 
Because SAC/AAC scores come from human judgment, they also more precisely capture the semantic relevance than JI and WES. 
In Figure~\ref{fig:manual}, we plot the SAC and AAC of the four approaches over the top-5 recommended results. 
We also plot the average SAC$\times$AAC, which reflects how high percent of $\langle$app, sentence$\rangle$ pairs (among top-5 results) contain both a relevant sentence and a relevant app. 
Here the parameters are fixed to $\lambda_1=\lambda_2=0.4$, $\lambda_3=\lambda_4=0.1$ and top-K = 20.  


\begin{figure}[t]
\vspace{-0.1in}
\centering
\subfloat{
\begin{tikzpicture} [scale=0.7]
\begin{groupplot}[group style={group size= 1 by 2},height=5cm,width=7cm]%[ybar stacked,xtick=\empty,]%ytick=\empty]
\nextgroupplot[ybar,symbolic x coords={3,7,13}, ybar=.13cm, legend style={at={(1.2, 0.65)},anchor=north}, enlarge x limits=0.25, ymin=0, ymax=1.0,ymajorgrids = true,bar width = 6.5,xtick=data,xticklabels={SAC, AAC, SAC$\times$AAC}]%ytick=\empty]
\addplot[fill=white,draw=black] 
coordinates {(3, 0.379)  (7, 0.826)  (13, 0.374)};
\addplot[fill=black,draw=black] 
coordinates {(3, 0.396)  (7, 0.745)  (13, 0.374)};
\addplot[fill=black,draw=black,pattern=dots] 
coordinates {(3, 0.447)  (7, 0.804)  (13, 0.430)};
\addplot[fill=black,draw=black,pattern=north west lines] 
coordinates {(3, 0.860)  (7, 0.906)  (13, 0.855)};
\addlegendentry{T+W}
\addlegendentry{R+K}
\addlegendentry{T+K}
\addlegendentry{CLAP}
\end{groupplot}
\end{tikzpicture}
}\\
\subfloat{
\begin{tikzpicture} [scale=0.7]
\begin{groupplot}[group style={group size= 1 by 2},height=5cm,width=7cm]%[ybar stacked,xtick=\empty,]%ytick=\empty]
\nextgroupplot[ybar,symbolic x coords={3,7,13}, ybar=.13cm, legend style={at={(1.2, 0.65)},anchor=north}, enlarge x limits=0.25, ymin=0, ymax=1.0,ymajorgrids = true,bar width = 6.5,xtick=data,xticklabels={SAC, AAC, SAC$\times$AAC}]%ytick=\empty]
\addplot[fill=white,draw=black] 
coordinates {(3, 0.587)  (7, 0.757)  (13, 0.583)};
\addplot[fill=black,draw=black] 
coordinates {(3, 0.557)  (7, 0.696)  (13, 0.530)};
\addplot[fill=black,draw=black,pattern=dots] 
coordinates {(3, 0.548)  (7, 0.765)  (13, 0.522)};
\addplot[fill=black,draw=black,pattern=north west lines] 
coordinates {(3, 0.857)  (7, 0.870)  (13, 0.813)};
\addlegendentry{T+W}
\addlegendentry{R+K}
\addlegendentry{T+K}
\addlegendentry{CLAP}
\end{groupplot}
\end{tikzpicture}
}
\caption{The quantitative evaluation results of manually-judged accuracy: bar plots show the average accuracy of top-5 results in each of the four approaches. The upper plot shows results on \textsf{CONTACT}$_{authr}$; the lower plot shows results on \textsf{RECORD}$_{authr}$; T-test between the highest and second highest scores in each group are 9e-7, 0.03, 9e-6 (upper) and 4e-6, 0.04, 1e-4 (lower). Parameter settings are $\lambda_1=\lambda_2=0.4$, $\lambda_3=\lambda_4=0.1$, top-K=20. \label{fig:manual}}
\vspace{-0.1in}
\end{figure}

{\bf Results Analysis}.  Figure~\ref{fig:manual} shows that CLAP has significantly better performance in all the three metrics. Given the results from Table~\ref{tab:result}, the SAC results are expectable; however, the AAC results are surprising. This serendipity comes from the fact that the baselines (T + K and T + W) follow the greedy technique of recommending the most similar apps, while sometimes those apps turn out to be less similar than the apps recommended by CLAP. 
Such result might indicate that CLAP has the potential to discover even more relevant apps. 

%''We further compute the category agreement probability in the two datasets, and \textsf{RECORD} has opposite results while \textsf{CONTACT} shows the same result as in Figure~\ref{fig:manual}, i.e., CLAP has better app accuracy than alternative approaches (CLAP: 0.43, T+K: 0.38, R+K: 0.34, T+W: 0.37).


%!TEX root =../main.tex

\subsection{Qualitative Evaluation}
\label{sec:quality}

We next present our qualitative evaluation on helping developers improve the interpretability of their permission explanations: (1) 
how interpretable are the sentences recommended by CLAP? 
(2) to what extent can these sentences help developers discover new permission requirements? 
Because it is difficult to answer these questions quantitatively, we inspect specific examples of the recommended sentences and examine their interpretability. 

Column 3 of Table~\ref{tab:casestudy} shows the sentences that CLAP recommends for three example apps. The three apps come from \textsf{CONTACTS}$_{dev}$, \textsf{RECORD}$_{dev}$, and \textsf{LOCATION}$_{dev}$,  respectively. 
For each app, Column 2 shows its title, description, and the gold-standard explaining sentence. 
Column 4 shows the top-voted words (based on Equation~\ref{sec:vote}, Section~\ref{sec:vote}). 
We show a word in bold if it overlaps with words in the recommended sentences or with the current app's description. 

From Table~\ref{tab:casestudy}, we observe the following three characteristics of the recommended sentences.

{\bf Diverse Choices of Phrasing}. 
We observe that the recommended sentences provide various rephrasing, e.g., ``\emph{to send a scheduled sms}'' vs. ``\emph{set the time to send message}'', allowing the developer to choose from a diverse vocabulary to improve the explanation. 
The reason why CLAP can support diverse wording choices is that it removes the duplicated sentences in the post-processing step (Section~\ref{sec:postprocess}). 

{\bf Detailed Purposes}. 
We observe that the sentences recommended by CLAP usually state concrete and detailed permission purposes. 
In contrast, the sentences recommended by the baselines often contain examples such as ``\emph{to read contacts},'' which does not mention any specific purpose. 
The reason why CLAP can recommend more detailed purposes is that it uses the inverse document frequency (IDF) for word voting (Section~\ref{sec:vote}). The IDF helps select the most meaningful words by demoting common and non-discriminative words~\cite{idf}. 
Indeed, we observe that words in Column 4 are good indicators of specific permission purposes. 

{\bf Concise Sentences}. 
We observe that the sentences recommended by CLAP are usually short and concise. 
This result is due to the fact that CLAP breaks long sentences into shorter ones. 
Both the long sentences and the shorter sentences are added to the candidate set (Section~\ref{sec:candidate}); 
however, it is easier for the shorter sentences to be highly voted, because a long sentence tends to contain infrequent words that some of its sub-sentences do not contain. 
Because the most voted words are frequent words, the shorter sentences are more likely to receive high votes. 

We further conduct a quantitative study on the lengths of the sentences recommended by CLAP and the baselines. 
We compute the average and maximum lengths of the recommended sentences over all the five test collections in Table~\ref{tab:stat}. 
We find that the average length of the CLAP-recommended sentences is less than 56\% of the second shortest average length (CLAP: 8.1; T + W: 14.6, T + K: 14.3, R + K: 15.6) while the maximum length of the CLAP-recommended sentences is less than 36\% of the second shortest maximum length (CLAP: 31, T + W: 174, T + K: 174, R + K: 86). 
Note that if a recommended sentence is as long as 174 words, it must be difficult for the developer to digest. 
Because conciseness is an important aspect of interpretability~\cite{conf/kdd/LakkarajuBL16}, sentences recommended by CLAP effectively improve the worst case of interpretability against the baselines. 

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!

\section{Limitations and Future Work}
\label{sec:discussion}

%[Tao to check]
% Cheng: This does sound like "limitations"; perhaps we want to first point out some limitation to better match the title of this section? (We started with a limitation in the next two paragraphs, so they are fine.)
% Cheng: It seems that we need an introductory paragraph here for this section especially because the first sentence isn't really discussing any limitation, or otherwise, we can start with a discussion of limitation in "User Study". 
In this section, we discuss the limitations of CLAP and future work.

\textbf{User Study}. One limitation of this work is that we have not had a systematic way to directly evaluate the interpretability of explanation sentences. 
In future work, we plan to investigate more direct evaluation than our current evaluation. In particular, we plan to measure the interpretability from an \emph{end-user}'s perspective, e.g., investigating the following  research questions: how often do explanations confuse average users? 
are there any general rules that developers could follow to improve the interpretability of permission explanations? 
how to effectively explain rare permission usages? 

\textbf{Availability of Similar Apps}. 
Because CLAP recommends sentences from similar apps' descriptions, its performance depends on 
both the availability of similar apps and the quality of similar apps' descriptions. 
If an app lacks enough similar apps, or if its similar apps are poorly explained, CLAP's performance will decrease. 
To improve CLAP's performance under such cases, we recommend using a larger dataset to increase the number of well-explained candidate sentences. 

\textbf{Checking Apps' Actual Behaviors}. 
In our current work, we measure the similarity between two apps by leveraging four components: the two apps' descriptions, their titles, their permissions, and their categories. 
Besides the four components, we can further check the Android API methods invoked by the two apps to observe whether these invoked API methods \emph{indeed} share the same permission purpose.  
One caveat is that CLAP cannot be used to detect over-privileged permissions; for such permissions, CLAP explains their usages in the same way as for legitimate permissions. 

%!TEX root =../main.tex

% Life's more fun when you live in the moment :) Happy Snapping!
\vspace{-0.1in}
\section{Related Work}
\label{sec:relwork}

%Our CLAP framework mines app description data for permission requirements discovery. We summarize related work in the following three directions.  

\textbf{Mining App Store Data for Requirements Engineering}. 
In recent years, the requirements engineering community has shown great interest in mining data from the Google Play app store~\cite{tian2015characteristics}, especially text data~\cite{conf/re/MasseyEAS13,bhatia2016mining,conf/re/EvansBWB17,guzman2014users}. 
App store data serves as a bridge between app developers and app users. 
On one hand, text data from the Play store (e.g., app descriptions, existing user reviews, and ratings) has a broad impact on users' decision-making process (e.g., whether to install an app, purchase an app, or give reviews and rating). On the other hand, such data provides important clues for guiding future development and requirements discovery. 

App description data can be used for requirements discovery tasks such as domain analysis~\cite{journals/tse/HaririCMCM13}, e.g., analyzing similar apps to discover their common and varied parts. App review data~\cite{harman2012app,conf/re/PaganoM13,conf/icse/CarrenoW13,guzman2014users,conf/re/MaalejN15,DBLP:conf/re/JohannSBM17}  contain rich user feedback information such as their sentiments toward existing features~\cite{guzman2014users}, future feature requirements~\cite{conf/icse/CarrenoW13}, and bug reports~\cite{conf/re/MaalejN15}. Privacy policy data can be mined to assist privacy requirements analysis~\cite{journals/re/AntonE04,conf/re/MasseyEAS13,bhatia2017data,bhatia2016mining,conf/re/EvansBWB17,conf/ndss/ZimmeckWZI0SWSB17,slavin2016toward}. 

\textbf{Explaining Android Permission}. 
Compared with targeted attacks, a more prevalent security issue in Android apps is the over-privileged problem~\cite{conf/ccs/FeltCHSW11}, i.e., apps using more permissions than they need. 
The study results by Felt et al.~\cite{conf/soups/FeltHEHCW12} show that users usually have a difficult time understanding why permissions are used. Lin et al.~\cite{conf/huc/LinSALHZ12,conf/soups/LinLSH14} examine users' expectations toward Android permissions. 
Their results reveal general security concerns toward permission usages; however, the security concerns can be alleviated by providing a natural language sentence to explain the permission purpose. 

Previous work has explored multiple approaches to explain an app's permission, e.g., using the app's description sentences~\cite{conf/uss/PanditaXYEX13,conf/ccs/QuRZCZC14}, a set of manually-annotated purposes~\cite{conf/huc/WangHG15}, pre-defined text templates~\cite{conf/ccs/ZhangDFY15}, or GUI mapping~\cite{conf/huc/LiGC16}. However, these previous approaches all assume that the permission explanations already exist in the app, and therefore these approaches cannot be used to discover new requirements. 
Our work fills this gap in the previous work by providing tool supports for recommending new permission requirements. 

\textbf{NLP for App Security}. In recent years, NLP techniques are widely applied to various security tasks~\cite{conf/icse/GorlaTGZ14,slavin2016toward}. CHABADA~\cite{conf/icse/GorlaTGZ14} uses the topic modeling technique and outlier detection techniques to discover potential malware within each app cluster. 
Slavin et al.~\cite{slavin2016toward} construct a knowledge hierarchy that joins security sensitive APIs with natural language concepts to detect violations of textual privacy policies. 
As follow-up work of WHYPER~\cite{conf/uss/PanditaXYEX13}, AutoCog~\cite{conf/ccs/QuRZCZC14} uses the app description to represent the most frequent permission purposes.

% but Yu et al.~\cite{conf/wcre/YuLQW16} later find that AutoCog produces many false positives, and propose to leverage an app's textual privacy policy and bytecode to reduce such false positives. 

%!TEX root =../main.tex

\section{Conclusion}
\label{sec:conclusion}

In this paper, we conduct the first study on the problem of permission requirements discovery for an Android app. 
When a developer needs to explain a permission usage in the app description, permission requirements discovery could help the developer find potential ways to improve the interpretability of permission explanations. 
We have proposed the CLAP framework for recommending  permission-explaining sentences from similar apps, based on  leveraging consensus among the most similar apps and selecting the sentences that best match the consensus. Our evaluation results have shown that CLAP can recommend sentences that are relevant, concise, include detailed purposes, and provide diverse choices of phrasing. 

\noindent \textbf{Acknowledgment}. This work was supported in part by NSF CNS-1513939, CNS-1408944,  CCF-1409423, and CNS-1564274. 