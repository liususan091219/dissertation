%!TEX root =../../thesis-ex.tex

\section{Related Work}
\label{sec:relwork}

The task of Natural Language Interface to Database (NLIDB) has received significant attention since the 1970s \cite{warren1982efficient,androutsopoulos1993masque,popescu2003towards,hallett2006generic}. Most of the early work focuses on the case where there is only one database being asked. For example, the famous ATIS dataset consists of 4,978 questions for the airline booking system. The Geo dataset consists of 880 questions on geographic facts, e.g., \emph{give me the cities in virginia}. These datasets are frequently used in existing work on natural language interface. It is therefore difficult to adapt such trained NLI to new domains, because with new database schemas, the relations will be different. 

Recently, more and more work has been focusing on the cross-domain scenario, including two large dataset Overnight~\cite{wang2015building} WikiSQL~\cite{zhong2017seq2sql} and Spider~\cite{yu2018spider}. The scale: the first dataset, Overnight consists of only 8 domains, each domain consists of close to 1K utterance; WikiSQL consists of the largest number of utterances and schemas. Way of construction: all three datasets are constructed using crowd sourcing, the difficulty of constructing a dataset is that the utterance and SQL statement must alight correctly. To achieve a large scale, the three datasets use difference strategies: Overnight leverage an alignment of natural language and SQL templates, e.g., \texttt{whose COLUMN (>|<|=...) VALUE} is the natural language template that corresponds to \texttt{WHERE COLUMN (>|<|=...) VALUE}. It first recursively automatically construct more complex templates based on rules in these templates. Then they enumerate compatible column names and value names for replacing \texttt{COLUMN} and \texttt{VALUE} in both template, and ask human to rephrase the natural language question for as many different versions as possible. In Overnight, each domain has its own training and testing fold, so the learner does not need to be able to adapt to new domain, but can learn the general grammar from the shared templates. 

Slightly different than overnight, WikiSQL first introduced the scenario where each database schema has just a few utterance, but there are 24K different schemas. It is possible that such scenario is inspired by the business intelligence need, where each customer may just ask a few questions to their uploaded database tables, but there can be a large number of such customers whose data can be used for training. It also introduce the scenario where the training schema and the testing schema are completely separated. An interesting question is what has been learned from the completely different schemas. A previous paper studies the domain adaptation and found a good performance~\cite{dadashkarimi2018zero}. 

WikiSQL is the largest dataset, but the sketch oversimplies the difficult question of NL2SQL. Because the database extracted from Wikipedia are more noisy, it may be more difficult to construct more complex queries. The dataset consists of only \texttt{SELECT} and \texttt{WHERE}, such queries can be randomly sampled from any table without a lot of constraint on the correctness, however, by randomly generated adding a set of columns to the same statement, the resulting statement might not be that meaningful. 

Spider is the latest large-scale dataset on NL2SQL. Different from WikiSQL, the dataset in Spider are pooled from multiple resources, including text book databases and the existing databases for single-domain NL2SQL tasks (e.g., Geo, ATIS, Yelp). The labeling jobs are done by a number of computer science students who are good at SQL, the natural language sentences are also manually created so the sentences look more natural. They rephrase each question a few times. 

After WikiSQL and Spider came out, the topic of NL2SQL has drawn quote some attentions. State-of-the-art has achieved 86\% accuracy on WikiSQL. Some of the ideas for improving WikiSQL include using the execution result as the feedback in reinforcement learning loop~\cite{zhong2017seq2sql}, leveraging a sketch~\cite{xu2017sqlnet}, first generating a sketch then fill the missing details~\cite{dong2018coarse}, column type~\cite{yu2018typesql}, using BERT to represent utterance and column names~\cite{hwang2019achieving}, and execution-guided decoding~\cite{wang2018execution}. In the Spider dataset, the state-of-the-art model is IRNet plus BERT~\cite{guo2019towards} achieving 64\% accuracy, other works include gated graph neural network~\cite{bogin2019representing}. 

Other work on natural language interface include translating the natural language utterance to the canonical natural language template, so that the template can directly be mapped to the corresponding SQL template~\cite{su2017cross}. The study this problem on the Overnight dataset, where they apply the word analogy property of word embedding to translate the utterance, e.g., in sentence \texttt{In which seasons did Kobe Bryant play for the Lakers?} and \texttt{When did Alice start working for Mckinsey?}, how \texttt{Kobe Bryant} to \texttt{Lakers} is analogous to how \texttt{Alice} is to \texttt{Mckinsey}. Such parallelism helps explain how sentence paradigms are being learned across different domains. Another work from the programming language community \cite{yaghmazadeh2017sqlizer} has used program repair and type system, where they initially leverage a general semantic parser, then the type of columns are checked against column values, if the two types are inconsistent, they use program repair techniques to fix the statement by enumerating the potentially correct column/values. 

